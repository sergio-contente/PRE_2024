{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# :tada: Out-of-Distribution (OOD) with PCA in Image Processing\n",
    "\n",
    "The goal of this notebook is to understand the depths of using Principal Component Analysis in order to perform OOD tasks in the domain of image processing.\n",
    "\n",
    "## :memo: Plan of action\n",
    "\n",
    "### :art: Preprocessing phase\n",
    "\n",
    "In order to achieve our goal, we need to understand how the dataset is structured.\n",
    "\n",
    "For this notebook, we are going to use the CBIR 15 dataset, that contains images of different places, such as an office, a bedroom, a mountain, etc. Note that there are some places that are similar one to another, i.e. a bedroom and a living room.\n",
    "\n",
    "Thus, in order to extract the features of the images we have to preprocess those images:\n",
    "\n",
    "- Get the images that are located in data/CBIR_15-scene and fit them to a dataframe using Pandas\n",
    "  - Locate the \"Labels.txt\" file: it shows where the indexes of the images from each category starts\n",
    "- Create the dataset with this information with two columns: the path to the image and its category\n",
    "- Transform all of the images in the same size (in this case, we are going with 256x256)\n",
    "  \n",
    "Now, in order to extract the features, it's necessary to divide the reshaped images into patches of 32x32 pixels. This is good to perform processing tasks to avoid waiting long periods of time.\n",
    "\n",
    "After all the preprocess, we should separate the images into two different foldes: one contains the patches of the training images that is going to give us their principal components and dimensions, and the other is the patches of the test images, that is going to be tested to fit into those dimensions and we'll get an OOD score afterwards.\n",
    "\n",
    "### :hammer: Training phase\n",
    "\n",
    "With the images that are stored inside the \"patches_train\" folder, the first thing we are going to do is _normalize_ all of the images to find the correct maximum covariance and transforming all the variables into the same scale.\n",
    "\n",
    "Next, we should then apply the PCA with all the components. As we have patches of 32x32, we'll be having 1024 features, hence components. Then we plot a graph to see how many components truly contributes for the most variance of the data - and give us more information about it. We're going to take the threshold of 95% of variance in this notebook.\n",
    "\n",
    "After getting the PCA with components that describe 95% of the variance, it's time to test our images and see how far of the residual space their data can be found.\n",
    "\n",
    "### :alembic: Test phase and results\n",
    "\n",
    "In this phase, we take the test images and normalize then with the same scale of each PCA. This is important to maintain consistency throughout the final results and measure the norms in the new dimension properly.\n",
    "\n",
    "After that, we calculate the norm of the projection of the given data into the orthogonal space of the principal component and divide it by the norm of the data in relation to the origin. This is the OOD score.\n",
    "\n",
    "We calculate the mean of the score for each category and get the minimal one. The current environment is the smallest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from dataframe_generator import *\n",
    "from image_patching import *"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
