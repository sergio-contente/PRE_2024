{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ Out-of-Distribution (OOD) with PCA using Deep Features from the Latent Space\n",
    "\n",
    "The goal of this notebook is to understand the depths of using Principal Component Analysis in order to perform OOD tasks using deep features from the latent space\n",
    "\n",
    "## üìù Plan of action\n",
    "\n",
    "### ‚ôªÔ∏è Preprocessing phase\n",
    "\n",
    "In order to achieve our goal, we need to understand how the dataset is structured.\n",
    "\n",
    "For this notebook, we are going to use the CBIR 15 dataset, that contains images of different places, such as an office, a bedroom, a mountain, etc. Note that there are some places that are similar one to another, i.e. a bedroom and a living room.\n",
    "\n",
    "Thus, in order to extract the features of the images we have to preprocess those images:\n",
    "\n",
    "- Get the images that are located in data/CBIR_15-scene and fit them to a dataframe using Pandas\n",
    "  - Locate the \"Labels.txt\" file: it shows where the indexes of the images from each category starts\n",
    "- Create the dataset with this information with two columns: the path to the image and its category\n",
    "- Transform all of the images in the same size (in this case, we are going with 256x256)\n",
    "  \n",
    "Now, in order to extract the features, it's necessary to divide the reshaped images into patches of 32x32 pixels. This is good to perform processing tasks to avoid waiting long periods of time.\n",
    "\n",
    "After all the preprocess, we should separate the images into two different foldes: one contains the patches of the training images that is going to give us their principal components and dimensions, and the other is the patches of the test images, that is going to be tested to fit into those dimensions and we'll get an OOD score afterwards.\n",
    "\n",
    "### üèãüèΩ‚Äç‚ôÇÔ∏è Training phase\n",
    "\n",
    "With the images that are stored inside the \"patches_train\" folder, the first thing we are going to do is _normalize_ all of the images to find the correct maximum covariance and transforming all the variables into the same scale.\n",
    "\n",
    "Next, we should then apply the PCA with all the components. As we have patches of 32x32, we'll be having 1024 features, hence components. Then we plot a graph to see how many components truly contributes for the most variance of the data - and give us more information about it. We're going to take the threshold of 95% of variance in this notebook.\n",
    "\n",
    "After getting the PCA with components that describe 95% of the variance, it's time to test our images and see how far of the residual space their data can be found.\n",
    "\n",
    "### ‚öóÔ∏è Test phase and results\n",
    "\n",
    "In this phase, we take the test images and normalize then with the same scale of each PCA. This is important to maintain consistency throughout the final results and measure the norms in the new dimension properly.\n",
    "\n",
    "After that, we calculate the norm of the projection of the given data into the orthogonal space of the principal component and divide it by the norm of the data in relation to the origin. This is the OOD score.\n",
    "\n",
    "We calculate the mean of the score for each category and get the minimal one. The current environment is the smallest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "First of all, we need to understand which libraries we are going to use:\n",
    "\n",
    "- os: Deals with the operation system interface such as finding the relative and absolute path of files inside a project and reading/writing files for example.\n",
    "- sys: This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.\n",
    "- numpy: NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\n",
    "- pandas: Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- matplotlib: Deals with plotting graphs to visualize data in a graphical way.\n",
    "- sklearn: Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd suggest to use a conda virtual environment in order to avoid messing up your base kernel environment and causing dependency errors in the future.\n",
    "\n",
    "After you successfully installed all the modules, it's time to import our custom modules that are going to deal with:\n",
    "\n",
    "- Creation of our dataframe using pandas\n",
    "- Separation of our dataset into patches of 32x32 in folders of training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from dataframe_generator import *\n",
    "from images_standardizing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_tgz(tgz_path, extract_to):\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "    \n",
    "    with tarfile.open(tgz_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "        print(f\"Arquivos extra√≠dos para {extract_to}\")\n",
    "\n",
    "tgz_path = '../CBIR_15-Scene.tgz'\n",
    "extract_to = '../data/'\n",
    "\n",
    "extract_tgz(tgz_path, extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òùÔ∏è Part I: Comparing two different environments\n",
    "\n",
    "### ‚ôªÔ∏è Preprocessing phase\n",
    "\n",
    "Now we start our experiments to understand if our idea work, however this time we are going to understand what happens with our approach using two different environments.\n",
    "\n",
    "In our case, I'm going to take the **Coast** and **Office** environments arbitrarily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = ['Coast', 'Office']\n",
    "\n",
    "df_different = df[df['category'].isin(train_categories)]\n",
    "df_different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to separate our dataset into train and test. We should use the built-in function of sklearn to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_different['image_path'].tolist()\n",
    "y = df_different['category'].tolist()\n",
    "unique_categories = list(df_different['category'].unique())\n",
    "print(f\"Unique categories: {unique_categories}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "standard_size = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that everything went well, we plot the grid of all the patches from the first image of our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what the module that's inside our \"image_patching.py\" do. So we now, need to save everything into the subfolders by calling that function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_images_set(X_train, X_test, y_train, y_test, output_dir_train='images_train', output_dir_test='images_test', standard_size=standard_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should load our patches for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_images_by_category = load_images_by_category('images_train', y, image_size=(224, 224))\n",
    "training_images_by_category = load_images_by_category('images_train', unique_categories, image_size=(224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_images(images):\n",
    "    # Calcular a m√©dia ao longo do eixo dos pixels\n",
    "    # Check if images have 3 or 4 dimensions\n",
    "    if len(images.shape) == 3:\n",
    "        num_images, height, width = images.shape\n",
    "        # For grayscale images, no need for the 'channels' dimension\n",
    "        mean_image = np.mean(images, axis=(1, 2), keepdims=True)\n",
    "    elif len(images.shape) == 4:\n",
    "        num_images, height, width, channels = images.shape\n",
    "        mean_image = np.mean(images, axis=(1, 2, 3), keepdims=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected image shape\")\n",
    "\n",
    "    # Subtract the mean from each image\n",
    "    centered_images = images - mean_image\n",
    "    \n",
    "    return centered_images\n",
    "\n",
    "centered_images_by_category = {}\n",
    "for category, images in training_images_by_category.items():\n",
    "    print(images.shape)\n",
    "    centered_images = center_images(images)\n",
    "    centered_images_by_category[category] = centered_images\n",
    "    print(f\"Category {category}, images shape: {centered_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãüèΩ‚Äç‚ôÇÔ∏è Training phase\n",
    "\n",
    "Now that the have our training patches stored in that variable above, we should start our analysis with PCA.\n",
    "\n",
    "First of all, we **need to normalize and center** the data. It's so importantt that I had to emphasize it. Plus, since we are dealing with different categories, each one of them should be normalized with a different scaler (and we're going to save it for later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images_by_category = centered_images_by_category \n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=True)\n",
    "base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the before last layer (Fully connected)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_grayscale_to_rgb(images):\n",
    "    return np.stack((images,) * 3, axis=-1)\n",
    "\n",
    "features_by_category = {}\n",
    "for category, images in preprocessed_images_by_category.items():\n",
    "    # Verifica se a imagem est√° em grayscale\n",
    "    if images.shape[-1] != 3:\n",
    "        images = convert_grayscale_to_rgb(images)\n",
    "    features = model.predict(images)\n",
    "    features_by_category[category] = features\n",
    "\n",
    "for category, features in features_by_category.items():\n",
    "    print(f\"Category {category}, features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_by_category = {}\n",
    "explained_variance_by_category = {}\n",
    "\n",
    "for category, features in features_by_category.items():\n",
    "    pca = PCA(n_components=0.95)  \n",
    "    principal_components = pca.fit_transform(features)\n",
    "    pca_by_category[category] = pca\n",
    "    explained_variance_by_category[category] = pca.explained_variance_ratio_\n",
    "    \n",
    "    print(f\"Category {category}, principal components: {principal_components.shape[1]}\")\n",
    "\n",
    "for category, pca in pca_by_category.items():\n",
    "    print(f\"Category {category}, principal components shape: {pca.components_.shape}\")\n",
    "    print(f\"Category {category}, explained variance: {np.sum(explained_variance_by_category[category]) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_test_images(test_dir, categories, image_size, input_size):\n",
    "    test_images_by_category = load_images_by_category(test_dir, categories, image_size)\n",
    "    test_centered_images_by_category = {}\n",
    "    test_scalers_by_category = {}\n",
    "\n",
    "    for category, images in test_images_by_category.items():\n",
    "        test_centered_images = center_images(images)\n",
    "        test_centered_images_by_category[category] = test_centered_images\n",
    "\n",
    "    return test_centered_images_by_category\n",
    "\n",
    "image_size = (224, 224)\n",
    "\n",
    "test_preprocessed_images_by_category = load_and_preprocess_test_images('images_test', y, image_size, input_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ensure_rgb_format(images):\n",
    "    # Verifique se as imagens t√™m tr√™s dimens√µes (batch_size, height, width)\n",
    "    if len(images.shape) == 3:  \n",
    "        # Se for grayscale, expanda a dimens√£o para criar imagens com 3 canais (RGB)\n",
    "        images = np.stack((images,) * 3, axis=-1)\n",
    "    return images\n",
    "\n",
    "def extract_features_with_vgg16(model, preprocessed_images_by_category):\n",
    "    features_by_category = {}\n",
    "    for category, images in preprocessed_images_by_category.items():\n",
    "        # Garanta que as imagens est√£o no formato RGB correto\n",
    "        images = ensure_rgb_format(images)\n",
    "        \n",
    "        # Realize a predi√ß√£o com o modelo\n",
    "        features = model.predict(images)\n",
    "        features_by_category[category] = features\n",
    "    return features_by_category\n",
    "\n",
    "# Agora extraia as caracter√≠sticas usando o modelo\n",
    "test_features_by_category = extract_features_with_vgg16(model, test_preprocessed_images_by_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_features(features_by_category):\n",
    "    centralized_features_by_category = {}\n",
    "    for category, features in features_by_category.items():\n",
    "        # Centralize as features subtraindo a m√©dia\n",
    "        mean_features = np.mean(features, axis=0)\n",
    "        centralized_features = features - mean_features\n",
    "        centralized_features_by_category[category] = centralized_features\n",
    "        \n",
    "        print(f\"Category {category}: centralized features shape = {centralized_features.shape}\")\n",
    "        print(f\"Category {category}: mean of centralized features = {np.mean(centralized_features, axis=0)}\")  # Deve estar pr√≥ximo de 0\n",
    "    return centralized_features_by_category\n",
    "\n",
    "centralized_test_features_by_category = centralize_features(test_features_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reconstruction_error(test_features, pca_by_category):\n",
    "    reconstruction_errors_by_category = {}\n",
    "    mean_reconstruction_errors_by_category = {}\n",
    "    \n",
    "    for category, pca in pca_by_category.items():\n",
    "        principal_components = pca.transform(test_features)\n",
    "        reconstructed_features = pca.inverse_transform(principal_components)\n",
    "        \n",
    "        reconstruction_error = np.linalg.norm(test_features - reconstructed_features, axis=1)\n",
    "        reconstruction_errors_by_category[category] = reconstruction_error / np.linalg.norm(test_features)\n",
    "\n",
    "    for category, errors in reconstruction_errors_by_category.items():\n",
    "        mean_reconstruction_errors_by_category[category] = np.mean(errors)\n",
    "    \n",
    "    best_category = min(mean_reconstruction_errors_by_category, key=mean_reconstruction_errors_by_category.get)\n",
    "\n",
    "    for category in mean_reconstruction_errors_by_category:\n",
    "        print(f\"Category {category}, mean reconstruction error: {mean_reconstruction_errors_by_category[category]}\")\n",
    "    \n",
    "    print(f\"Best category: {best_category}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    return mean_reconstruction_errors_by_category, best_category\n",
    "\n",
    "for category, test_features in centralized_test_features_by_category.items():\n",
    "    print(f\"Test category: {category}\")\n",
    "    mean_reconstruction_errors, best_category = calculate_reconstruction_error(test_features, pca_by_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agnostic Spaces Analsys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar o dicion√°rio para armazenar os resultados de PCA\n",
    "pca_results = {}\n",
    "\n",
    "# Lista de percentuais de vari√¢ncia explicada para os quais voc√™ quer calcular\n",
    "percentages = [95]\n",
    "categories = ['Coast', 'Office']\n",
    "\n",
    "# Loop atrav√©s de diferentes percentuais de vari√¢ncia explicada\n",
    "for perc in percentages:\n",
    "    # Inicializar dicion√°rios para armazenar os resultados\n",
    "    pca_by_category = {}\n",
    "    explained_variance_by_category = {}\n",
    "\n",
    "    # Loop atrav√©s de categorias\n",
    "    for category, features in features_by_category.items():\n",
    "        # Inicializar PCA com a porcentagem especificada\n",
    "        pca = PCA(n_components=perc / 100.0)\n",
    "        principal_components = pca.fit_transform(features)\n",
    "        \n",
    "        # Armazenar os resultados do PCA para cada categoria\n",
    "        pca_by_category[category] = pca\n",
    "        explained_variance_by_category[category] = pca.explained_variance_ratio_\n",
    "        \n",
    "        print(f\"Category {category}, principal components: {principal_components.shape[1]}\")\n",
    "\n",
    "    # Armazenar resultados no dicion√°rio principal pca_results\n",
    "    pca_results[perc] = {}\n",
    "    for category in categories:\n",
    "        if category in pca_by_category:\n",
    "            pca = pca_by_category[category]\n",
    "            components = pca.components_\n",
    "            explained_variance_ratio = pca.explained_variance_ratio_\n",
    "            \n",
    "            # Armazenar os componentes, a vari√¢ncia explicada e o objeto PCA\n",
    "            pca_results[perc][category] = {\n",
    "                'components': components,\n",
    "                'explained_variance_ratio': explained_variance_ratio,\n",
    "                'pca_object': pca\n",
    "            }\n",
    "            \n",
    "            print(f\"Category {category}, principal components shape: {components.shape}\")\n",
    "            print(f\"Category {category}, explained variance: {np.sum(explained_variance_ratio) * 100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Categoria '{category}' n√£o est√° presente nos dados para {perc}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_variance(explained_variance_ratio):\n",
    "    \"\"\"\n",
    "    Plota a vari√¢ncia explicada acumulada com base na vari√¢ncia explicada de cada componente principal.\n",
    "    \n",
    "    Parameters:\n",
    "    - explained_variance_ratio: Array ou lista contendo a vari√¢ncia explicada por componente.\n",
    "    \"\"\"\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)  # Calcula a vari√¢ncia acumulada\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se a estrutura de pca_results cont√©m a vari√¢ncia explicada\n",
    "percentage_to_use = 95  # Escolha a porcentagem que deseja usar\n",
    "category_to_use = 'Coast'  # Escolha a categoria para analisar\n",
    "\n",
    "if percentage_to_use in pca_results:\n",
    "    if category_to_use in pca_results[percentage_to_use]:\n",
    "        explained_variance_ratio = pca_results[percentage_to_use][category_to_use].get('explained_variance_ratio', None)\n",
    "        \n",
    "        if explained_variance_ratio is not None:\n",
    "            plot_cumulative_variance(explained_variance_ratio)\n",
    "        else:\n",
    "            print(f\"Explained variance ratio not found for {category_to_use} at {percentage_to_use}%.\")\n",
    "    else:\n",
    "        print(f\"Category '{category_to_use}' not found for {percentage_to_use}%.\")\n",
    "else:\n",
    "    print(f\"Percentage '{percentage_to_use}%' not found in pca_results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se a estrutura de pca_results cont√©m a vari√¢ncia explicada\n",
    "percentage_to_use = 95  # Escolha a porcentagem que deseja usar\n",
    "category_to_use = 'Office'  # Escolha a categoria para analisar\n",
    "\n",
    "if percentage_to_use in pca_results:\n",
    "    if category_to_use in pca_results[percentage_to_use]:\n",
    "        explained_variance_ratio = pca_results[percentage_to_use][category_to_use].get('explained_variance_ratio', None)\n",
    "        \n",
    "        if explained_variance_ratio is not None:\n",
    "            plot_cumulative_variance(explained_variance_ratio)\n",
    "        else:\n",
    "            print(f\"Explained variance ratio not found for {category_to_use} at {percentage_to_use}%.\")\n",
    "    else:\n",
    "        print(f\"Category '{category_to_use}' not found for {percentage_to_use}%.\")\n",
    "else:\n",
    "    print(f\"Percentage '{percentage_to_use}%' not found in pca_results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fun√ß√£o para projetar patches nos componentes de uma categoria\n",
    "def project_test_patches(patches, pca_components):\n",
    "    # Certifique-se de que patches tem pelo menos 2 dimens√µes\n",
    "    if len(patches.shape) == 1:\n",
    "        patches = np.expand_dims(patches, axis=0)  # Expande para (1, n_features)\n",
    "    \n",
    "    return np.dot(patches, pca_components.T)\n",
    "\n",
    "# Fun√ß√£o para calcular normas, m√©dias e m√©dias das normas dos produtos internos no mesmo componente\n",
    "def calculate_norms_and_means(projections_A, projections_B):\n",
    "    norms = []\n",
    "    means = []\n",
    "    means_norms = []\n",
    "\n",
    "    # Verificar se projections_A e projections_B t√™m pelo menos duas dimens√µes\n",
    "    if len(projections_A.shape) < 2 or len(projections_B.shape) < 2:\n",
    "        raise ValueError(\"Projections must have at least two dimensions (patches, components).\")\n",
    "\n",
    "    for i in range(projections_A.shape[1]):  # Itera sobre os componentes\n",
    "        dot_products = np.dot(projections_A[:, i], projections_B[:, i].T)\n",
    "        norms.append(np.linalg.norm(dot_products))\n",
    "        means.append(np.mean(dot_products))\n",
    "        means_norms.append(np.mean(np.linalg.norm(dot_products)))\n",
    "        \n",
    "    return norms, means, means_norms\n",
    "\n",
    "# Fun√ß√£o para plotar as normas m√©dias para todas as imagens combinadas\n",
    "def plot_mean_norms_for_all_images(category, other_category, mean_of_means_norms, color='blue'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plotar os resultados para todos os componentes combinados em todas as imagens\n",
    "    plt.bar(range(len(mean_of_means_norms)), mean_of_means_norms, color=color,\n",
    "            label=f'{category} on {other_category} - All Images')\n",
    "    \n",
    "    plt.title(f'Mean of Norms for Components ({category} on {other_category}) - 95% Variance Explained')\n",
    "    plt.xlabel('Component Index')\n",
    "    plt.ylabel('Mean of Norms')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Lista de categorias para iterar\n",
    "categories = ['Coast', 'Office']\n",
    "\n",
    "# Trabalhando apenas com o PCA de 95% de vari√¢ncia explicada\n",
    "perc = 95\n",
    "\n",
    "# Iterar sobre as categorias para calcular tanto intra-categories quanto cross-categories\n",
    "for category in categories:\n",
    "    for other_category in categories:\n",
    "        # Carregar os componentes da pr√≥pria categoria ou da outra categoria\n",
    "        components = pca_results[perc][other_category]['components']\n",
    "        \n",
    "        if category == other_category:\n",
    "            print(f\"\\nCategory: {category} on {category} (Intra-Category), Percentage: {perc}%\")\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            print(f\"\\nCategory: {category} on {other_category} (Cross-Category), Percentage: {perc}%\")\n",
    "            color = 'green'\n",
    "        \n",
    "        # Armazenar as normas m√©dias para todas as imagens\n",
    "        all_means_norms = []\n",
    "        \n",
    "        # Proje√ß√£o dos patches (intra ou cross-categoria) para todas as imagens\n",
    "        # Itera sobre cada imagem no array de features\n",
    "        for image_idx in range(centralized_test_features_by_category[category].shape[0]):\n",
    "            patches = centralized_test_features_by_category[category][image_idx]  # Seleciona os patches da imagem\n",
    "\n",
    "            # Projeta os patches nos componentes\n",
    "            projected_patches = project_test_patches(patches, components)\n",
    "            \n",
    "            # Verifique a forma de projected_patches\n",
    "            if len(projected_patches.shape) < 2 or projected_patches.shape[1] != components.shape[0]:\n",
    "                raise ValueError(f\"Projected patches have unexpected shape: {projected_patches.shape}. Expected at least 2 dimensions and components matching PCA.\")\n",
    "            \n",
    "            # Calcular normas, m√©dias e m√©dias das normas para cada imagem\n",
    "            _, _, means_norms_category = calculate_norms_and_means(projected_patches, projected_patches)\n",
    "            \n",
    "            # Armazenar as normas calculadas para a imagem\n",
    "            all_means_norms.append(means_norms_category)\n",
    "        \n",
    "        # Calcular a m√©dia das normas para todas as imagens\n",
    "        mean_of_means_norms = np.mean(all_means_norms, axis=0)  # M√©dia das normas em todas as imagens\n",
    "        \n",
    "        # Plotar os valores m√©dios das normas para todos os componentes combinados\n",
    "        plot_mean_norms_for_all_images(category, other_category, mean_of_means_norms, color=color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fun√ß√£o para projetar patches nos componentes de uma categoria\n",
    "def project_test_patches(patches, pca_components):\n",
    "    projected = np.dot(patches, pca_components.T)\n",
    "    \n",
    "    # Verifica se o resultado √© unidimensional e expande para duas dimens√µes, se necess√°rio\n",
    "    if len(projected.shape) == 1:\n",
    "        projected = np.expand_dims(projected, axis=0)\n",
    "    \n",
    "    return projected\n",
    "\n",
    "# Fun√ß√£o para calcular normas, m√©dias e m√©dias das normas dos produtos internos no mesmo componente\n",
    "def calculate_norms_and_means(projections_A, projections_B):\n",
    "    norms = []\n",
    "    means = []\n",
    "    means_norms = []\n",
    "\n",
    "    # Certifique-se de que projections_A tenha pelo menos duas dimens√µes\n",
    "    if len(projections_A.shape) < 2:\n",
    "        raise ValueError(\"projections_A must have at least two dimensions (patches, components).\")\n",
    "    \n",
    "    for i in range(projections_A.shape[1]):  # Itera sobre os componentes\n",
    "        dot_products = np.dot(projections_A[:, i], projections_B[:, i].T)\n",
    "        norms.append(np.linalg.norm(dot_products))\n",
    "        means.append(np.mean(dot_products))\n",
    "        means_norms.append(np.mean(np.linalg.norm(dot_products)))\n",
    "        \n",
    "    return norms, means, means_norms\n",
    "\n",
    "# Fun√ß√£o para capturar componentes que explicam ~90% da vari√¢ncia e cujas normas est√£o abaixo do percentil desejado e de um limite superior\n",
    "def capture_components_by_percentile_and_threshold(explained_variance_ratio, means_norms, variance_threshold=0.9, exclude_first=True, norm_threshold=50):\n",
    "    # Calcular a vari√¢ncia explicada cumulativa\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # Capturar os √≠ndices que explicam at√© ~90% da vari√¢ncia\n",
    "    selected_indices = np.where(cumulative_variance <= variance_threshold)[0]\n",
    "    \n",
    "    # Excluir a primeira componente se necess√°rio\n",
    "    if exclude_first and 0 in selected_indices:\n",
    "        selected_indices = selected_indices[selected_indices != 0]\n",
    "    \n",
    "    # Garantir que selected_indices seja uma lista de inteiros\n",
    "    selected_indices = list(map(int, selected_indices))\n",
    "    \n",
    "    # Calcular o percentil desejado dos means_norms\n",
    "    percentile = np.percentile([means_norms[i] for i in selected_indices], 7)\n",
    "    \n",
    "    # Selecionar os componentes com means_norms abaixo do percentil e do limite superior\n",
    "    selected_indices_filtered = [i for i in selected_indices if means_norms[i] <= percentile and means_norms[i] <= norm_threshold]\n",
    "    \n",
    "    return selected_indices_filtered\n",
    "\n",
    "# Lista de categorias para iterar\n",
    "categories = ['Coast', 'Office']\n",
    "\n",
    "# Trabalhando apenas com o PCA de 95% de vari√¢ncia explicada\n",
    "perc = 95\n",
    "\n",
    "selected_indices_dict = {}\n",
    "\n",
    "# Iterar sobre as categorias para calcular tanto intra-categories quanto cross-categories\n",
    "for category in categories:\n",
    "    for other_category in categories:\n",
    "        # Carregar os componentes e vari√¢ncia explicada da pr√≥pria categoria ou da outra categoria\n",
    "        components = pca_results[perc][other_category]['components']\n",
    "        explained_variance_ratio = pca_results[perc][other_category]['explained_variance_ratio']\n",
    "        \n",
    "        if category == other_category:\n",
    "            print(f\"\\nCategory: {category} on {category} (Intra-Category), Percentage: {perc}%\")\n",
    "        else:\n",
    "            print(f\"\\nCategory: {category} on {other_category} (Cross-Category), Percentage: {perc}%\")\n",
    "        \n",
    "        # Proje√ß√£o dos patches (intra ou cross-categoria)\n",
    "        all_means_norms = []\n",
    "        all_selected_indices = []\n",
    "        \n",
    "        # Iterando pelas imagens e patches\n",
    "        for image_idx in range(centralized_test_features_by_category[category].shape[0]):\n",
    "            patches = centralized_test_features_by_category[category][image_idx]  # Seleciona os patches da imagem\n",
    "\n",
    "            # Projeta os patches nos componentes\n",
    "            projected_patches = project_test_patches(patches, components)\n",
    "            \n",
    "            # Calcular normas, m√©dias e m√©dias das normas para cada imagem\n",
    "            norms_category, means_category, means_norms_category = calculate_norms_and_means(projected_patches, projected_patches)\n",
    "            \n",
    "            # Capturar os componentes cujas m√©dias das normas est√£o abaixo do percentil e explicam at√© ~90% da vari√¢ncia\n",
    "            selected_indices = capture_components_by_percentile_and_threshold(explained_variance_ratio, means_norms_category, exclude_first=True, norm_threshold=50)\n",
    "            \n",
    "            # Armazenar os resultados de m√©dias das normas e componentes selecionados\n",
    "            all_means_norms.append(means_norms_category)\n",
    "            all_selected_indices.append(selected_indices)\n",
    "        \n",
    "        # Verifique se existem componentes selecionados\n",
    "        if len(all_selected_indices) == 0 or np.concatenate(all_selected_indices).size == 0:\n",
    "            print(f\"Warning: No components selected for {category} on {other_category}. Skipping this combination.\")\n",
    "            continue\n",
    "\n",
    "        # Agregue os componentes selecionados em todas as imagens\n",
    "        aggregated_selected_indices = np.unique(np.concatenate(all_selected_indices)).astype(int)  # Convers√£o para inteiros\n",
    "        \n",
    "        # Inicializar os dicion√°rios se as chaves n√£o existirem\n",
    "        if category not in selected_indices_dict:\n",
    "            selected_indices_dict[category] = {}\n",
    "        \n",
    "        selected_indices_dict[category][other_category] = aggregated_selected_indices\n",
    "\n",
    "        # Evite plotagens se n√£o houver componentes selecionados\n",
    "        if len(aggregated_selected_indices) == 0:\n",
    "            print(f\"Warning: No valid components selected for {category} on {other_category}. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        # Plotar os resultados para os componentes selecionados\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(aggregated_selected_indices, [np.mean([means_norms[int(i)] for means_norms in all_means_norms if int(i) < len(means_norms)]) for i in aggregated_selected_indices], \n",
    "                color='green' if category != other_category else 'blue',\n",
    "                label=f'{category} on {other_category} - Selected Components')\n",
    "        plt.title(f'Selected Components Based on ~90% Variance and Below 7th Percentile of Mean Norms ({category} on {other_category}) - 95% Variance Explained')\n",
    "        plt.xlabel('Component Index')\n",
    "        plt.ylabel('Mean of Norms')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_test_office_patches = centralized_test_features_by_category['Office']\n",
    "centered_test_coast_patches = centralized_test_features_by_category['Coast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def project_and_transform_back(features, pca, specific_indices):\n",
    "    \"\"\"\n",
    "    Projeta as features nos componentes principais espec√≠ficos e reconstr√≥i a partir desses componentes.\n",
    "    \"\"\"\n",
    "    # Proje√ß√£o das features nos componentes principais\n",
    "    projected = pca.transform(features)\n",
    "    \n",
    "    # Usar apenas os componentes espec√≠ficos\n",
    "    projected_specific = projected[:, specific_indices]\n",
    "    \n",
    "    # Reconstruir as features apenas com os componentes espec√≠ficos\n",
    "    specific_components = pca.components_[specific_indices]\n",
    "    reconstructed_features = np.dot(projected_specific, specific_components)\n",
    "    \n",
    "    return reconstructed_features\n",
    "\n",
    "def calculate_mean_ood_for_specific_components(original_features, pca, specific_indices):\n",
    "    \"\"\"\n",
    "    Projeta as features originais em componentes PCA espec√≠ficos, reconstr√≥i e calcula a m√©dia dos OOD scores.\n",
    "    \"\"\"\n",
    "    total_ood_scores = []\n",
    "    \n",
    "    # Itera sobre todas as amostras de features\n",
    "    for sample_idx in range(original_features.shape[0]):\n",
    "        features = original_features[sample_idx]\n",
    "        \n",
    "        # Proje√ß√£o e reconstru√ß√£o das features nos componentes espec√≠ficos\n",
    "        reconstructed_features = project_and_transform_back(features.reshape(1, -1), pca, specific_indices)\n",
    "        \n",
    "        # Calcula os res√≠duos (erro de reconstru√ß√£o)\n",
    "        residuals = features - reconstructed_features.flatten()\n",
    "        \n",
    "        # Calcular a norma das features originais e dos res√≠duos\n",
    "        original_norm = np.linalg.norm(features)\n",
    "        residual_norm = np.linalg.norm(residuals)\n",
    "        \n",
    "        # Verifique se a norma dos res√≠duos √© maior que a norma das features originais\n",
    "        if residual_norm > original_norm:\n",
    "            print(f\"Warning: Residual norm ({residual_norm}) greater than original norm ({original_norm}) for sample {sample_idx}\")\n",
    "        \n",
    "        # Calcular a pontua√ß√£o OOD (norma dos res√≠duos sobre a norma das features originais)\n",
    "        if original_norm == 0:\n",
    "            ood_score = 0\n",
    "        else:\n",
    "            ood_score = residual_norm / original_norm\n",
    "        \n",
    "        # Adiciona a pontua√ß√£o OOD desta amostra √† lista total\n",
    "        total_ood_scores.append(ood_score)\n",
    "    \n",
    "    # Retorna a m√©dia das pontua√ß√µes OOD\n",
    "    return np.mean(total_ood_scores)\n",
    "\n",
    "# Iterar sobre as categorias para calcular as m√©dias das pontua√ß√µes OOD\n",
    "mean_ood_scores = {}\n",
    "\n",
    "for category in categories:\n",
    "    for other_category in categories:\n",
    "        specific_indices = selected_indices_dict[category][other_category]\n",
    "        \n",
    "        # Recupera os objetos PCA para as categorias correspondentes\n",
    "        pca_object = pca_results[perc][other_category]['pca_object']  # Usamos os componentes do other_category\n",
    "        \n",
    "        # Verificar se as features de teste existem para a categoria\n",
    "        if category not in centralized_test_features_by_category:\n",
    "            print(f\"Warning: No test features found for {category}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Calcular a m√©dia das pontua√ß√µes OOD com base na proje√ß√£o nos componentes espec√≠ficos\n",
    "        mean_ood = calculate_mean_ood_for_specific_components(centralized_test_features_by_category[category], pca_object, specific_indices)\n",
    "        \n",
    "        # Armazenar a m√©dia no dicion√°rio\n",
    "        mean_ood_scores[f\"{category}_on_{other_category}\"] = mean_ood\n",
    "\n",
    "# Exibir todas as m√©dias calculadas\n",
    "for key, mean_ood in mean_ood_scores.items():\n",
    "    print(f\"Mean OOD Score for {key}: {mean_ood}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úåÔ∏è Part II: Comparing two similar environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = ['Bedroom', 'LivingRoom']\n",
    "\n",
    "df_different = df[df['category'].isin(train_categories)]\n",
    "df_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_different['image_path']\n",
    "y = df_different['category']\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "image_size = (224, 224)\n",
    "unique_categories = list(df_different['category'].unique())\n",
    "print(f\"Unique categories: {unique_categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_images_set(X_train, X_test, y_train, y_test, output_dir_train='images_train', output_dir_test='images_test', standard_size=standard_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_by_category = load_images_by_category('images_train', unique_categories, image_size=(224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_images(images):\n",
    "    # Calcular a m√©dia ao longo do eixo dos pixels\n",
    "    # Check if images have 3 or 4 dimensions\n",
    "    if len(images.shape) == 3:\n",
    "        num_images, height, width = images.shape\n",
    "        # For grayscale images, no need for the 'channels' dimension\n",
    "        mean_image = np.mean(images, axis=(1, 2), keepdims=True)\n",
    "    elif len(images.shape) == 4:\n",
    "        num_images, height, width, channels = images.shape\n",
    "        mean_image = np.mean(images, axis=(1, 2, 3), keepdims=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected image shape\")\n",
    "\n",
    "    # Subtract the mean from each image\n",
    "    centered_images = images - mean_image\n",
    "    \n",
    "    return centered_images\n",
    "\n",
    "centered_images_by_category = {}\n",
    "for category, images in training_images_by_category.items():\n",
    "    print(images.shape)\n",
    "    centered_images = center_images(images)\n",
    "    centered_images_by_category[category] = centered_images\n",
    "    print(f\"Category {category}, images shape: {centered_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_grayscale_to_rgb(images):\n",
    "    return np.stack((images,) * 3, axis=-1)\n",
    "\n",
    "preprocessed_images_by_category = centered_images_by_category \n",
    "features_by_category = {}\n",
    "for category, images in preprocessed_images_by_category.items():\n",
    "    # Verifica se a imagem est√° em grayscale\n",
    "    if images.shape[-1] != 3:\n",
    "        images = convert_grayscale_to_rgb(images)\n",
    "    features = model.predict(images)\n",
    "    features_by_category[category] = features\n",
    "\n",
    "\n",
    "for category, features in features_by_category.items():\n",
    "    print(f\"Category {category}, features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_by_category = {}\n",
    "explained_variance_by_category = {}\n",
    "\n",
    "for category, features in features_by_category.items():\n",
    "    pca = PCA(n_components=0.95)  \n",
    "    principal_components = pca.fit_transform(features)\n",
    "    pca_by_category[category] = pca\n",
    "    explained_variance_by_category[category] = pca.explained_variance_ratio_\n",
    "    \n",
    "    print(f\"Category {category}, principal components: {principal_components.shape[1]}\")\n",
    "\n",
    "for category, pca in pca_by_category.items():\n",
    "    print(f\"Category {category}, principal components shape: {pca.components_.shape}\")\n",
    "    print(f\"Category {category}, explained variance: {np.sum(explained_variance_by_category[category]) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_test_images(test_dir, categories, image_size, input_size):\n",
    "    test_images_by_category = load_images_by_category(test_dir, categories, image_size)\n",
    "    test_centered_images_by_category = {}\n",
    "    test_scalers_by_category = {}\n",
    "\n",
    "    for category, images in test_images_by_category.items():\n",
    "        test_centered_images = center_images(images)\n",
    "        test_centered_images_by_category[category] = test_centered_images\n",
    "\n",
    "    return test_centered_images_by_category\n",
    "\n",
    "image_size = (224, 224)\n",
    "\n",
    "test_preprocessed_images_by_category = load_and_preprocess_test_images('images_test', y, image_size, input_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ensure_rgb_format(images):\n",
    "    # Verifique se as imagens t√™m tr√™s dimens√µes (batch_size, height, width)\n",
    "    if len(images.shape) == 3:  \n",
    "        # Se for grayscale, expanda a dimens√£o para criar imagens com 3 canais (RGB)\n",
    "        images = np.stack((images,) * 3, axis=-1)\n",
    "    return images\n",
    "\n",
    "def extract_features_with_vgg16(model, preprocessed_images_by_category):\n",
    "    features_by_category = {}\n",
    "    for category, images in preprocessed_images_by_category.items():\n",
    "        # Garanta que as imagens est√£o no formato RGB correto\n",
    "        images = ensure_rgb_format(images)\n",
    "        \n",
    "        # Realize a predi√ß√£o com o modelo\n",
    "        features = model.predict(images)\n",
    "        features_by_category[category] = features\n",
    "    return features_by_category\n",
    "\n",
    "# Agora extraia as caracter√≠sticas usando o modelo\n",
    "test_features_by_category = extract_features_with_vgg16(model, test_preprocessed_images_by_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_features(features_by_category):\n",
    "    centralized_features_by_category = {}\n",
    "    for category, features in features_by_category.items():\n",
    "        # Centralize as features subtraindo a m√©dia\n",
    "        mean_features = np.mean(features, axis=0)\n",
    "        centralized_features = features - mean_features\n",
    "        centralized_features_by_category[category] = centralized_features\n",
    "        \n",
    "        print(f\"Category {category}: centralized features shape = {centralized_features.shape}\")\n",
    "        print(f\"Category {category}: mean of centralized features = {np.mean(centralized_features, axis=0)}\")  # Deve estar pr√≥ximo de 0\n",
    "    return centralized_features_by_category\n",
    "\n",
    "centralized_test_features_by_category = centralize_features(test_features_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reconstruction_error(test_features, pca_by_category):\n",
    "    reconstruction_errors_by_category = {}\n",
    "    mean_reconstruction_errors_by_category = {}\n",
    "    \n",
    "    for category, pca in pca_by_category.items():\n",
    "        principal_components = pca.transform(test_features)\n",
    "        reconstructed_features = pca.inverse_transform(principal_components)\n",
    "        \n",
    "        reconstruction_error = np.linalg.norm(test_features - reconstructed_features, axis=1)\n",
    "        reconstruction_errors_by_category[category] = reconstruction_error / np.linalg.norm(test_features)\n",
    "\n",
    "    for category, errors in reconstruction_errors_by_category.items():\n",
    "        mean_reconstruction_errors_by_category[category] = np.mean(errors)\n",
    "    \n",
    "    best_category = min(mean_reconstruction_errors_by_category, key=mean_reconstruction_errors_by_category.get)\n",
    "\n",
    "    for category in mean_reconstruction_errors_by_category:\n",
    "        print(f\"Category {category}, mean reconstruction error: {mean_reconstruction_errors_by_category[category]}\")\n",
    "    \n",
    "    print(f\"Best category: {best_category}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    return mean_reconstruction_errors_by_category, best_category\n",
    "\n",
    "for category, test_features in centralized_test_features_by_category.items():\n",
    "    print(f\"Test category: {category}\")\n",
    "    mean_reconstruction_errors, best_category = calculate_reconstruction_error(test_features, pca_by_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agnostic Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar o dicion√°rio para armazenar os resultados de PCA\n",
    "pca_results = {}\n",
    "\n",
    "# Lista de percentuais de vari√¢ncia explicada para os quais voc√™ quer calcular\n",
    "percentages = [95]\n",
    "categories = ['Bedroom', 'LivingRoom']\n",
    "\n",
    "# Loop atrav√©s de diferentes percentuais de vari√¢ncia explicada\n",
    "for perc in percentages:\n",
    "    # Inicializar dicion√°rios para armazenar os resultados\n",
    "    pca_by_category = {}\n",
    "    explained_variance_by_category = {}\n",
    "\n",
    "    # Loop atrav√©s de categorias\n",
    "    for category, features in features_by_category.items():\n",
    "        # Inicializar PCA com a porcentagem especificada\n",
    "        pca = PCA(n_components=perc / 100.0)\n",
    "        principal_components = pca.fit_transform(features)\n",
    "        \n",
    "        # Armazenar os resultados do PCA para cada categoria\n",
    "        pca_by_category[category] = pca\n",
    "        explained_variance_by_category[category] = pca.explained_variance_ratio_\n",
    "        \n",
    "        print(f\"Category {category}, principal components: {principal_components.shape[1]}\")\n",
    "\n",
    "    # Armazenar resultados no dicion√°rio principal pca_results\n",
    "    pca_results[perc] = {}\n",
    "    for category in categories:\n",
    "        if category in pca_by_category:\n",
    "            pca = pca_by_category[category]\n",
    "            components = pca.components_\n",
    "            explained_variance_ratio = pca.explained_variance_ratio_\n",
    "            \n",
    "            # Armazenar os componentes, a vari√¢ncia explicada e o objeto PCA\n",
    "            pca_results[perc][category] = {\n",
    "                'components': components,\n",
    "                'explained_variance_ratio': explained_variance_ratio,\n",
    "                'pca_object': pca\n",
    "            }\n",
    "            \n",
    "            print(f\"Category {category}, principal components shape: {components.shape}\")\n",
    "            print(f\"Category {category}, explained variance: {np.sum(explained_variance_ratio) * 100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Categoria '{category}' n√£o est√° presente nos dados para {perc}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se a estrutura de pca_results cont√©m a vari√¢ncia explicada\n",
    "percentage_to_use = 95  # Escolha a porcentagem que deseja usar\n",
    "category_to_use = 'Bedroom'  # Escolha a categoria para analisar\n",
    "\n",
    "if percentage_to_use in pca_results:\n",
    "    if category_to_use in pca_results[percentage_to_use]:\n",
    "        explained_variance_ratio = pca_results[percentage_to_use][category_to_use].get('explained_variance_ratio', None)\n",
    "        \n",
    "        if explained_variance_ratio is not None:\n",
    "            plot_cumulative_variance(explained_variance_ratio)\n",
    "        else:\n",
    "            print(f\"Explained variance ratio not found for {category_to_use} at {percentage_to_use}%.\")\n",
    "    else:\n",
    "        print(f\"Category '{category_to_use}' not found for {percentage_to_use}%.\")\n",
    "else:\n",
    "    print(f\"Percentage '{percentage_to_use}%' not found in pca_results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se a estrutura de pca_results cont√©m a vari√¢ncia explicada\n",
    "percentage_to_use = 95  # Escolha a porcentagem que deseja usar\n",
    "category_to_use = 'LivingRoom'  # Escolha a categoria para analisar\n",
    "\n",
    "if percentage_to_use in pca_results:\n",
    "    if category_to_use in pca_results[percentage_to_use]:\n",
    "        explained_variance_ratio = pca_results[percentage_to_use][category_to_use].get('explained_variance_ratio', None)\n",
    "        \n",
    "        if explained_variance_ratio is not None:\n",
    "            plot_cumulative_variance(explained_variance_ratio)\n",
    "        else:\n",
    "            print(f\"Explained variance ratio not found for {category_to_use} at {percentage_to_use}%.\")\n",
    "    else:\n",
    "        print(f\"Category '{category_to_use}' not found for {percentage_to_use}%.\")\n",
    "else:\n",
    "    print(f\"Percentage '{percentage_to_use}%' not found in pca_results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fun√ß√£o para projetar patches nos componentes de uma categoria\n",
    "def project_test_patches(patches, pca_components):\n",
    "    # Certifique-se de que patches tem pelo menos 2 dimens√µes\n",
    "    if len(patches.shape) == 1:\n",
    "        patches = np.expand_dims(patches, axis=0)  # Expande para (1, n_features)\n",
    "    \n",
    "    return np.dot(patches, pca_components.T)\n",
    "\n",
    "# Fun√ß√£o para calcular normas, m√©dias e m√©dias das normas dos produtos internos no mesmo componente\n",
    "def calculate_norms_and_means(projections_A, projections_B):\n",
    "    norms = []\n",
    "    means = []\n",
    "    means_norms = []\n",
    "\n",
    "    # Verificar se projections_A e projections_B t√™m pelo menos duas dimens√µes\n",
    "    if len(projections_A.shape) < 2 or len(projections_B.shape) < 2:\n",
    "        raise ValueError(\"Projections must have at least two dimensions (patches, components).\")\n",
    "\n",
    "    for i in range(projections_A.shape[1]):  # Itera sobre os componentes\n",
    "        dot_products = np.dot(projections_A[:, i], projections_B[:, i].T)\n",
    "        norms.append(np.linalg.norm(dot_products))\n",
    "        means.append(np.mean(dot_products))\n",
    "        means_norms.append(np.mean(np.linalg.norm(dot_products)))\n",
    "        \n",
    "    return norms, means, means_norms\n",
    "\n",
    "# Fun√ß√£o para plotar as normas m√©dias para todas as imagens combinadas\n",
    "def plot_mean_norms_for_all_images(category, other_category, mean_of_means_norms, color='blue'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plotar os resultados para todos os componentes combinados em todas as imagens\n",
    "    plt.bar(range(len(mean_of_means_norms)), mean_of_means_norms, color=color,\n",
    "            label=f'{category} on {other_category} - All Images')\n",
    "    \n",
    "    plt.title(f'Mean of Norms for Components ({category} on {other_category}) - 95% Variance Explained')\n",
    "    plt.xlabel('Component Index')\n",
    "    plt.ylabel('Mean of Norms')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Lista de categorias para iterar\n",
    "categories = ['Bedroom', 'LivingRoom']\n",
    "\n",
    "# Trabalhando apenas com o PCA de 95% de vari√¢ncia explicada\n",
    "perc = 95\n",
    "\n",
    "# Iterar sobre as categorias para calcular tanto intra-categories quanto cross-categories\n",
    "for category in categories:\n",
    "    for other_category in categories:\n",
    "        # Carregar os componentes da pr√≥pria categoria ou da outra categoria\n",
    "        components = pca_results[perc][other_category]['components']\n",
    "        \n",
    "        if category == other_category:\n",
    "            print(f\"\\nCategory: {category} on {category} (Intra-Category), Percentage: {perc}%\")\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            print(f\"\\nCategory: {category} on {other_category} (Cross-Category), Percentage: {perc}%\")\n",
    "            color = 'green'\n",
    "        \n",
    "        # Armazenar as normas m√©dias para todas as imagens\n",
    "        all_means_norms = []\n",
    "        \n",
    "        # Proje√ß√£o dos patches (intra ou cross-categoria) para todas as imagens\n",
    "        # Itera sobre cada imagem no array de features\n",
    "        for image_idx in range(centralized_test_features_by_category[category].shape[0]):\n",
    "            patches = centralized_test_features_by_category[category][image_idx]  # Seleciona os patches da imagem\n",
    "\n",
    "            # Projeta os patches nos componentes\n",
    "            projected_patches = project_test_patches(patches, components)\n",
    "            \n",
    "            # Verifique a forma de projected_patches\n",
    "            if len(projected_patches.shape) < 2 or projected_patches.shape[1] != components.shape[0]:\n",
    "                raise ValueError(f\"Projected patches have unexpected shape: {projected_patches.shape}. Expected at least 2 dimensions and components matching PCA.\")\n",
    "            \n",
    "            # Calcular normas, m√©dias e m√©dias das normas para cada imagem\n",
    "            _, _, means_norms_category = calculate_norms_and_means(projected_patches, projected_patches)\n",
    "            \n",
    "            # Armazenar as normas calculadas para a imagem\n",
    "            all_means_norms.append(means_norms_category)\n",
    "        \n",
    "        # Calcular a m√©dia das normas para todas as imagens\n",
    "        mean_of_means_norms = np.mean(all_means_norms, axis=0)  # M√©dia das normas em todas as imagens\n",
    "        \n",
    "        # Plotar os valores m√©dios das normas para todos os componentes combinados\n",
    "        plot_mean_norms_for_all_images(category, other_category, mean_of_means_norms, color=color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fun√ß√£o para projetar patches nos componentes de uma categoria\n",
    "def project_test_patches(patches, pca_components):\n",
    "    projected = np.dot(patches, pca_components.T)\n",
    "    \n",
    "    # Verifica se o resultado √© unidimensional e expande para duas dimens√µes, se necess√°rio\n",
    "    if len(projected.shape) == 1:\n",
    "        projected = np.expand_dims(projected, axis=0)\n",
    "    \n",
    "    return projected\n",
    "\n",
    "# Fun√ß√£o para calcular normas, m√©dias e m√©dias das normas dos produtos internos no mesmo componente\n",
    "def calculate_norms_and_means(projections_A, projections_B):\n",
    "    norms = []\n",
    "    means = []\n",
    "    means_norms = []\n",
    "\n",
    "    # Certifique-se de que projections_A tenha pelo menos duas dimens√µes\n",
    "    if len(projections_A.shape) < 2:\n",
    "        raise ValueError(\"projections_A must have at least two dimensions (patches, components).\")\n",
    "    \n",
    "    for i in range(projections_A.shape[1]):  # Itera sobre os componentes\n",
    "        dot_products = np.dot(projections_A[:, i], projections_B[:, i].T)\n",
    "        norms.append(np.linalg.norm(dot_products))\n",
    "        means.append(np.mean(dot_products))\n",
    "        means_norms.append(np.mean(np.linalg.norm(dot_products)))\n",
    "        \n",
    "    return norms, means, means_norms\n",
    "\n",
    "# Fun√ß√£o para capturar componentes que explicam ~90% da vari√¢ncia e cujas normas est√£o abaixo do percentil desejado e de um limite superior\n",
    "def capture_components_by_percentile_and_threshold(explained_variance_ratio, means_norms, variance_threshold=0.9, exclude_first=True, norm_threshold=50):\n",
    "    # Calcular a vari√¢ncia explicada cumulativa\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # Capturar os √≠ndices que explicam at√© ~90% da vari√¢ncia\n",
    "    selected_indices = np.where(cumulative_variance <= variance_threshold)[0]\n",
    "    \n",
    "    # Excluir a primeira componente se necess√°rio\n",
    "    if exclude_first and 0 in selected_indices:\n",
    "        selected_indices = selected_indices[selected_indices != 0]\n",
    "    \n",
    "    # Garantir que selected_indices seja uma lista de inteiros\n",
    "    selected_indices = list(map(int, selected_indices))\n",
    "    \n",
    "    # Calcular o percentil desejado dos means_norms\n",
    "    percentile = np.percentile([means_norms[i] for i in selected_indices], 7)\n",
    "    \n",
    "    # Selecionar os componentes com means_norms abaixo do percentil e do limite superior\n",
    "    selected_indices_filtered = [i for i in selected_indices if means_norms[i] <= percentile and means_norms[i] <= norm_threshold]\n",
    "    \n",
    "    return selected_indices_filtered\n",
    "\n",
    "# Lista de categorias para iterar\n",
    "categories = ['Bedroom', 'LivingRoom']\n",
    "\n",
    "# Trabalhando apenas com o PCA de 95% de vari√¢ncia explicada\n",
    "perc = 95\n",
    "\n",
    "selected_indices_dict = {}\n",
    "\n",
    "# Iterar sobre as categorias para calcular tanto intra-categories quanto cross-categories\n",
    "for category in categories:\n",
    "    for other_category in categories:\n",
    "        # Carregar os componentes e vari√¢ncia explicada da pr√≥pria categoria ou da outra categoria\n",
    "        components = pca_results[perc][other_category]['components']\n",
    "        explained_variance_ratio = pca_results[perc][other_category]['explained_variance_ratio']\n",
    "        \n",
    "        if category == other_category:\n",
    "            print(f\"\\nCategory: {category} on {category} (Intra-Category), Percentage: {perc}%\")\n",
    "        else:\n",
    "            print(f\"\\nCategory: {category} on {other_category} (Cross-Category), Percentage: {perc}%\")\n",
    "        \n",
    "        # Proje√ß√£o dos patches (intra ou cross-categoria)\n",
    "        all_means_norms = []\n",
    "        all_selected_indices = []\n",
    "        \n",
    "        # Iterando pelas imagens e patches\n",
    "        for image_idx in range(centralized_test_features_by_category[category].shape[0]):\n",
    "            patches = centralized_test_features_by_category[category][image_idx]  # Seleciona os patches da imagem\n",
    "\n",
    "            # Projeta os patches nos componentes\n",
    "            projected_patches = project_test_patches(patches, components)\n",
    "            \n",
    "            # Calcular normas, m√©dias e m√©dias das normas para cada imagem\n",
    "            norms_category, means_category, means_norms_category = calculate_norms_and_means(projected_patches, projected_patches)\n",
    "            \n",
    "            # Capturar os componentes cujas m√©dias das normas est√£o abaixo do percentil e explicam at√© ~90% da vari√¢ncia\n",
    "            selected_indices = capture_components_by_percentile_and_threshold(explained_variance_ratio, means_norms_category, exclude_first=True, norm_threshold=50)\n",
    "            \n",
    "            # Armazenar os resultados de m√©dias das normas e componentes selecionados\n",
    "            all_means_norms.append(means_norms_category)\n",
    "            all_selected_indices.append(selected_indices)\n",
    "        \n",
    "        # Verifique se existem componentes selecionados\n",
    "        if len(all_selected_indices) == 0 or np.concatenate(all_selected_indices).size == 0:\n",
    "            print(f\"Warning: No components selected for {category} on {other_category}. Skipping this combination.\")\n",
    "            continue\n",
    "\n",
    "        # Agregue os componentes selecionados em todas as imagens\n",
    "        aggregated_selected_indices = np.unique(np.concatenate(all_selected_indices)).astype(int)  # Convers√£o para inteiros\n",
    "        \n",
    "        # Inicializar os dicion√°rios se as chaves n√£o existirem\n",
    "        if category not in selected_indices_dict:\n",
    "            selected_indices_dict[category] = {}\n",
    "        \n",
    "        selected_indices_dict[category][other_category] = aggregated_selected_indices\n",
    "\n",
    "        # Evite plotagens se n√£o houver componentes selecionados\n",
    "        if len(aggregated_selected_indices) == 0:\n",
    "            print(f\"Warning: No valid components selected for {category} on {other_category}. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        # Plotar os resultados para os componentes selecionados\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(aggregated_selected_indices, [np.mean([means_norms[int(i)] for means_norms in all_means_norms if int(i) < len(means_norms)]) for i in aggregated_selected_indices], \n",
    "                color='green' if category != other_category else 'blue',\n",
    "                label=f'{category} on {other_category} - Selected Components')\n",
    "        plt.title(f'Selected Components Based on ~90% Variance and Below 7th Percentile of Mean Norms ({category} on {other_category}) - 95% Variance Explained')\n",
    "        plt.xlabel('Component Index')\n",
    "        plt.ylabel('Mean of Norms')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def project_and_transform_back(features, pca, specific_indices):\n",
    "    \"\"\"\n",
    "    Projeta as features nos componentes principais espec√≠ficos e reconstr√≥i a partir desses componentes.\n",
    "    \"\"\"\n",
    "    # Proje√ß√£o das features nos componentes principais\n",
    "    projected = pca.transform(features)\n",
    "    \n",
    "    # Usar apenas os componentes espec√≠ficos\n",
    "    projected_specific = projected[:, specific_indices]\n",
    "    \n",
    "    # Reconstruir as features apenas com os componentes espec√≠ficos\n",
    "    specific_components = pca.components_[specific_indices]\n",
    "    reconstructed_features = np.dot(projected_specific, specific_components)\n",
    "    \n",
    "    return reconstructed_features\n",
    "\n",
    "def calculate_mean_ood_for_specific_components(original_features, pca, specific_indices):\n",
    "    \"\"\"\n",
    "    Projeta as features originais em componentes PCA espec√≠ficos, reconstr√≥i e calcula a m√©dia dos OOD scores.\n",
    "    \"\"\"\n",
    "    total_ood_scores = []\n",
    "    \n",
    "    # Itera sobre todas as amostras de features\n",
    "    for sample_idx in range(original_features.shape[0]):\n",
    "        features = original_features[sample_idx]\n",
    "        \n",
    "        # Proje√ß√£o e reconstru√ß√£o das features nos componentes espec√≠ficos\n",
    "        reconstructed_features = project_and_transform_back(features.reshape(1, -1), pca, specific_indices)\n",
    "        \n",
    "        # Calcula os res√≠duos (erro de reconstru√ß√£o)\n",
    "        residuals = features - reconstructed_features.flatten()\n",
    "        \n",
    "        # Calcular a norma das features originais e dos res√≠duos\n",
    "        original_norm = np.linalg.norm(features)\n",
    "        residual_norm = np.linalg.norm(residuals)\n",
    "        \n",
    "        # Verifique se a norma dos res√≠duos √© maior que a norma das features originais\n",
    "        if residual_norm > original_norm:\n",
    "            print(f\"Warning: Residual norm ({residual_norm}) greater than original norm ({original_norm}) for sample {sample_idx}\")\n",
    "        \n",
    "        # Calcular a pontua√ß√£o OOD (norma dos res√≠duos sobre a norma das features originais)\n",
    "        if original_norm == 0:\n",
    "            ood_score = 0\n",
    "        else:\n",
    "            ood_score = residual_norm / original_norm\n",
    "        \n",
    "        # Adiciona a pontua√ß√£o OOD desta amostra √† lista total\n",
    "        total_ood_scores.append(ood_score)\n",
    "    \n",
    "    # Retorna a m√©dia das pontua√ß√µes OOD\n",
    "    return np.mean(total_ood_scores)\n",
    "\n",
    "# Iterar sobre as categorias para calcular as m√©dias das pontua√ß√µes OOD\n",
    "mean_ood_scores = {}\n",
    "\n",
    "for category in categories:\n",
    "    for other_category in categories:\n",
    "        specific_indices = selected_indices_dict[category][other_category]\n",
    "        \n",
    "        # Recupera os objetos PCA para as categorias correspondentes\n",
    "        pca_object = pca_results[perc][other_category]['pca_object']  # Usamos os componentes do other_category\n",
    "        \n",
    "        # Verificar se as features de teste existem para a categoria\n",
    "        if category not in centralized_test_features_by_category:\n",
    "            print(f\"Warning: No test features found for {category}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Calcular a m√©dia das pontua√ß√µes OOD com base na proje√ß√£o nos componentes espec√≠ficos\n",
    "        mean_ood = calculate_mean_ood_for_specific_components(centralized_test_features_by_category[category], pca_object, specific_indices)\n",
    "        \n",
    "        # Armazenar a m√©dia no dicion√°rio\n",
    "        mean_ood_scores[f\"{category}_on_{other_category}\"] = mean_ood\n",
    "\n",
    "# Exibir todas as m√©dias calculadas\n",
    "for key, mean_ood in mean_ood_scores.items():\n",
    "    print(f\"Mean OOD Score for {key}: {mean_ood}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['image_path'].tolist()\n",
    "y = df['category'].tolist()\n",
    "unique_categories = list(df['category'].unique())\n",
    "print(f\"Unique categories: {unique_categories}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "standard_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_images_set(X_train, X_test, y_train, y_test, output_dir_train='images_train', output_dir_test='images_test', standard_size=standard_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_by_category = load_images_by_category('images_train', unique_categories, image_size=(224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_images(images):\n",
    "    # Calcular a m√©dia ao longo do eixo dos pixels\n",
    "    # Check if images have 3 or 4 dimensions\n",
    "    if len(images.shape) == 3:\n",
    "        num_images, height, width = images.shape\n",
    "        # For grayscale images, no need for the 'channels' dimension\n",
    "        mean_image = np.mean(images, axis=(1, 2), keepdims=True)\n",
    "    elif len(images.shape) == 4:\n",
    "        num_images, height, width, channels = images.shape\n",
    "        mean_image = np.mean(images, axis=(1, 2, 3), keepdims=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected image shape\")\n",
    "\n",
    "    # Subtract the mean from each image\n",
    "    centered_images = images - mean_image\n",
    "    \n",
    "    return centered_images\n",
    "\n",
    "centered_images_by_category = {}\n",
    "for category, images in training_images_by_category.items():\n",
    "    print(images.shape)\n",
    "    centered_images = center_images(images)\n",
    "    centered_images_by_category[category] = centered_images\n",
    "    print(f\"Category {category}, images shape: {centered_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_grayscale_to_rgb(images):\n",
    "    return np.stack((images,) * 3, axis=-1)\n",
    "\n",
    "preprocessed_images_by_category = centered_images_by_category \n",
    "features_by_category = {}\n",
    "for category, images in preprocessed_images_by_category.items():\n",
    "    # Verifica se a imagem est√° em grayscale\n",
    "    if images.shape[-1] != 3:\n",
    "        images = convert_grayscale_to_rgb(images)\n",
    "    features = model.predict(images)\n",
    "    features_by_category[category] = features\n",
    "\n",
    "\n",
    "for category, features in features_by_category.items():\n",
    "    print(f\"Category {category}, features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_by_category = {}\n",
    "explained_variance_by_category = {}\n",
    "\n",
    "for category, features in features_by_category.items():\n",
    "    pca = PCA(n_components=0.95)  \n",
    "    principal_components = pca.fit_transform(features)\n",
    "    pca_by_category[category] = pca\n",
    "    explained_variance_by_category[category] = pca.explained_variance_ratio_\n",
    "    \n",
    "    print(f\"Category {category}, principal components: {principal_components.shape[1]}\")\n",
    "\n",
    "for category, pca in pca_by_category.items():\n",
    "    print(f\"Category {category}, principal components shape: {pca.components_.shape}\")\n",
    "    print(f\"Category {category}, explained variance: {np.sum(explained_variance_by_category[category]) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_test_images(test_dir, categories, image_size, input_size):\n",
    "    test_images_by_category = load_images_by_category(test_dir, categories, image_size)\n",
    "    test_centered_images_by_category = {}\n",
    "    test_scalers_by_category = {}\n",
    "\n",
    "    for category, images in test_images_by_category.items():\n",
    "        test_centered_images = center_images(images)\n",
    "        test_centered_images_by_category[category] = test_centered_images\n",
    "\n",
    "    return test_centered_images_by_category\n",
    "\n",
    "image_size = (224, 224)\n",
    "\n",
    "test_preprocessed_images_by_category = load_and_preprocess_test_images('images_test', y, image_size, input_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ensure_rgb_format(images):\n",
    "    # Verifique se as imagens t√™m tr√™s dimens√µes (batch_size, height, width)\n",
    "    if len(images.shape) == 3:  \n",
    "        # Se for grayscale, expanda a dimens√£o para criar imagens com 3 canais (RGB)\n",
    "        images = np.stack((images,) * 3, axis=-1)\n",
    "    return images\n",
    "\n",
    "def extract_features_with_vgg16(model, preprocessed_images_by_category):\n",
    "    features_by_category = {}\n",
    "    for category, images in preprocessed_images_by_category.items():\n",
    "        # Garanta que as imagens est√£o no formato RGB correto\n",
    "        images = ensure_rgb_format(images)\n",
    "        \n",
    "        # Realize a predi√ß√£o com o modelo\n",
    "        features = model.predict(images)\n",
    "        features_by_category[category] = features\n",
    "    return features_by_category\n",
    "\n",
    "# Agora extraia as caracter√≠sticas usando o modelo\n",
    "test_features_by_category = extract_features_with_vgg16(model, test_preprocessed_images_by_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralize_features(features_by_category):\n",
    "    centralized_features_by_category = {}\n",
    "    for category, features in features_by_category.items():\n",
    "        # Centralize as features subtraindo a m√©dia\n",
    "        mean_features = np.mean(features, axis=0)\n",
    "        centralized_features = features - mean_features\n",
    "        centralized_features_by_category[category] = centralized_features\n",
    "        \n",
    "        print(f\"Category {category}: centralized features shape = {centralized_features.shape}\")\n",
    "        print(f\"Category {category}: mean of centralized features = {np.mean(centralized_features, axis=0)}\")  # Deve estar pr√≥ximo de 0\n",
    "    return centralized_features_by_category\n",
    "\n",
    "centralized_test_features_by_category = centralize_features(test_features_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reconstruction_error(test_features, pca_by_category):\n",
    "    reconstruction_errors_by_category = {}\n",
    "    mean_reconstruction_errors_by_category = {}\n",
    "    \n",
    "    for category, pca in pca_by_category.items():\n",
    "        principal_components = pca.transform(test_features)\n",
    "        reconstructed_features = pca.inverse_transform(principal_components)\n",
    "        \n",
    "        reconstruction_error = np.linalg.norm(test_features - reconstructed_features, axis=1)\n",
    "        reconstruction_errors_by_category[category] = reconstruction_error / np.linalg.norm(test_features)\n",
    "\n",
    "    for category, errors in reconstruction_errors_by_category.items():\n",
    "        mean_reconstruction_errors_by_category[category] = np.mean(errors)\n",
    "    \n",
    "    best_category = min(mean_reconstruction_errors_by_category, key=mean_reconstruction_errors_by_category.get)\n",
    "\n",
    "    for category in mean_reconstruction_errors_by_category:\n",
    "        print(f\"Category {category}, mean reconstruction error: {mean_reconstruction_errors_by_category[category]}\")\n",
    "    \n",
    "    print(f\"Best category: {best_category}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    return mean_reconstruction_errors_by_category, best_category\n",
    "\n",
    "for category, test_features in centralized_test_features_by_category.items():\n",
    "    print(f\"Test category: {category}\")\n",
    "    mean_reconstruction_errors, best_category = calculate_reconstruction_error(test_features, pca_by_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Inicializar o dicion√°rio para armazenar os resultados de PCA\n",
    "pca_results = {}\n",
    "\n",
    "# Lista de percentuais de vari√¢ncia explicada para os quais voc√™ quer calcular\n",
    "percentages = [95]\n",
    "categories = ['Bedroom', 'Suburb', 'Industry', 'Kitchen', 'LivingRoom', 'Coast', 'Forest', \n",
    "              'Highway', 'InsideCity', 'Mountain', 'OpenCountry', 'Street', 'Building', \n",
    "              'Office', 'Store']\n",
    "\n",
    "# Loop atrav√©s de diferentes percentuais de vari√¢ncia explicada\n",
    "for perc in percentages:\n",
    "    # Inicializar dicion√°rios para armazenar os resultados\n",
    "    pca_by_category = {}\n",
    "    explained_variance_by_category = {}\n",
    "\n",
    "    # Loop atrav√©s de categorias\n",
    "    for category, features in features_by_category.items():\n",
    "        # Inicializar PCA com a porcentagem especificada\n",
    "        pca = PCA(n_components=perc / 100.0)\n",
    "        principal_components = pca.fit_transform(features)\n",
    "        \n",
    "        # Armazenar os resultados do PCA para cada categoria\n",
    "        pca_by_category[category] = pca\n",
    "        explained_variance_by_category[category] = pca.explained_variance_ratio_\n",
    "        \n",
    "        print(f\"Category {category}, principal components: {principal_components.shape[1]}\")\n",
    "\n",
    "    # Armazenar resultados no dicion√°rio principal pca_results\n",
    "    pca_results[perc] = {}\n",
    "    for category in categories:\n",
    "        if category in pca_by_category:\n",
    "            pca = pca_by_category[category]\n",
    "            components = pca.components_\n",
    "            explained_variance_ratio = pca.explained_variance_ratio_\n",
    "            \n",
    "            # Armazenar os componentes, a vari√¢ncia explicada e o objeto PCA\n",
    "            pca_results[perc][category] = {\n",
    "                'components': components,\n",
    "                'explained_variance_ratio': explained_variance_ratio,\n",
    "                'pca_object': pca\n",
    "            }\n",
    "            \n",
    "            print(f\"Category {category}, principal components shape: {components.shape}\")\n",
    "            print(f\"Category {category}, explained variance: {np.sum(explained_variance_ratio) * 100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Categoria '{category}' n√£o est√° presente nos dados para {perc}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fun√ß√£o para projetar patches nos componentes de uma categoria\n",
    "def project_test_patches(patches, pca_components):\n",
    "    # Certifique-se de que patches tem pelo menos 2 dimens√µes\n",
    "    if len(patches.shape) == 1:\n",
    "        patches = np.expand_dims(patches, axis=0)  # Expande para (1, n_features)\n",
    "    \n",
    "    return np.dot(patches, pca_components.T)\n",
    "\n",
    "# Fun√ß√£o para calcular normas, m√©dias e m√©dias das normas dos produtos internos no mesmo componente\n",
    "def calculate_norms_and_means(projections_A, projections_B):\n",
    "    norms = []\n",
    "    means = []\n",
    "    means_norms = []\n",
    "\n",
    "    # Verificar se projections_A e projections_B t√™m pelo menos duas dimens√µes\n",
    "    if len(projections_A.shape) < 2 or len(projections_B.shape) < 2:\n",
    "        raise ValueError(\"Projections must have at least two dimensions (patches, components).\")\n",
    "\n",
    "    for i in range(projections_A.shape[1]):  # Itera sobre os componentes\n",
    "        dot_products = np.dot(projections_A[:, i], projections_B[:, i].T)\n",
    "        norms.append(np.linalg.norm(dot_products))\n",
    "        means.append(np.mean(dot_products))\n",
    "        means_norms.append(np.mean(np.linalg.norm(dot_products)))\n",
    "        \n",
    "    return norms, means, means_norms\n",
    "\n",
    "# Fun√ß√£o para plotar as normas m√©dias para todas as imagens combinadas\n",
    "def plot_mean_norms_for_all_images(category, other_category, mean_of_means_norms, color='blue'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plotar os resultados para todos os componentes combinados em todas as imagens\n",
    "    plt.bar(range(len(mean_of_means_norms)), mean_of_means_norms, color=color,\n",
    "            label=f'{category} on {other_category} - All Images')\n",
    "    \n",
    "    plt.title(f'Mean of Norms for Components ({category} on {other_category}) - 95% Variance Explained')\n",
    "    plt.xlabel('Component Index')\n",
    "    plt.ylabel('Mean of Norms')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Lista de categorias para iterar\n",
    "categories = ['Bedroom', 'Suburb', 'Industry', 'Kitchen', 'LivingRoom', 'Coast', 'Forest', \n",
    "              'Highway', 'InsideCity', 'Mountain', 'OpenCountry', 'Street', 'Building', \n",
    "              'Office', 'Store']\n",
    "\n",
    "# Trabalhando apenas com o PCA de 95% de vari√¢ncia explicada\n",
    "perc = 95\n",
    "\n",
    "# Iterar sobre as categorias para calcular tanto intra-categories quanto cross-categories\n",
    "for category in categories:\n",
    "    for other_category in categories:\n",
    "        # Carregar os componentes da pr√≥pria categoria ou da outra categoria\n",
    "        components = pca_results[perc][other_category]['components']\n",
    "        \n",
    "        if category == other_category:\n",
    "            print(f\"\\nCategory: {category} on {category} (Intra-Category), Percentage: {perc}%\")\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            print(f\"\\nCategory: {category} on {other_category} (Cross-Category), Percentage: {perc}%\")\n",
    "            color = 'green'\n",
    "        \n",
    "        # Armazenar as normas m√©dias para todas as imagens\n",
    "        all_means_norms = []\n",
    "        \n",
    "        # Proje√ß√£o dos patches (intra ou cross-categoria) para todas as imagens\n",
    "        # Itera sobre cada imagem no array de features\n",
    "        for image_idx in range(centralized_test_features_by_category[category].shape[0]):\n",
    "            patches = centralized_test_features_by_category[category][image_idx]  # Seleciona os patches da imagem\n",
    "\n",
    "            # Projeta os patches nos componentes\n",
    "            projected_patches = project_test_patches(patches, components)\n",
    "            \n",
    "            # Verifique a forma de projected_patches\n",
    "            if len(projected_patches.shape) < 2 or projected_patches.shape[1] != components.shape[0]:\n",
    "                raise ValueError(f\"Projected patches have unexpected shape: {projected_patches.shape}. Expected at least 2 dimensions and components matching PCA.\")\n",
    "            \n",
    "            # Calcular normas, m√©dias e m√©dias das normas para cada imagem\n",
    "            _, _, means_norms_category = calculate_norms_and_means(projected_patches, projected_patches)\n",
    "            \n",
    "            # Armazenar as normas calculadas para a imagem\n",
    "            all_means_norms.append(means_norms_category)\n",
    "        \n",
    "        # Calcular a m√©dia das normas para todas as imagens\n",
    "        mean_of_means_norms = np.mean(all_means_norms, axis=0)  # M√©dia das normas em todas as imagens\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fun√ß√£o para projetar patches nos componentes de uma categoria\n",
    "def project_test_patches(patches, pca_components):\n",
    "    projected = np.dot(patches, pca_components.T)\n",
    "    \n",
    "    # Verifica se o resultado √© unidimensional e expande para duas dimens√µes, se necess√°rio\n",
    "    if len(projected.shape) == 1:\n",
    "        projected = np.expand_dims(projected, axis=0)\n",
    "    \n",
    "    return projected\n",
    "\n",
    "# Fun√ß√£o para calcular normas, m√©dias e m√©dias das normas dos produtos internos no mesmo componente\n",
    "def calculate_norms_and_means(projections_A, projections_B):\n",
    "    norms = []\n",
    "    means = []\n",
    "    means_norms = []\n",
    "\n",
    "    # Certifique-se de que projections_A tenha pelo menos duas dimens√µes\n",
    "    if len(projections_A.shape) < 2:\n",
    "        raise ValueError(\"projections_A must have at least two dimensions (patches, components).\")\n",
    "    \n",
    "    for i in range(projections_A.shape[1]):  # Itera sobre os componentes\n",
    "        dot_products = np.dot(projections_A[:, i], projections_B[:, i].T)\n",
    "        norms.append(np.linalg.norm(dot_products))\n",
    "        means.append(np.mean(dot_products))\n",
    "        means_norms.append(np.mean(np.linalg.norm(dot_products)))\n",
    "        \n",
    "    return norms, means, means_norms\n",
    "\n",
    "# Fun√ß√£o para capturar componentes que explicam ~90% da vari√¢ncia e cujas normas est√£o abaixo do percentil desejado e de um limite superior\n",
    "def capture_components_by_percentile_and_threshold(explained_variance_ratio, means_norms, variance_threshold=0.9, exclude_first=True, norm_threshold=50):\n",
    "    # Calcular a vari√¢ncia explicada cumulativa\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    # Capturar os √≠ndices que explicam at√© ~90% da vari√¢ncia\n",
    "    selected_indices = np.where(cumulative_variance <= variance_threshold)[0]\n",
    "    \n",
    "    # Excluir a primeira componente se necess√°rio\n",
    "    if exclude_first and 0 in selected_indices:\n",
    "        selected_indices = selected_indices[selected_indices != 0]\n",
    "    \n",
    "    # Garantir que selected_indices seja uma lista de inteiros\n",
    "    selected_indices = list(map(int, selected_indices))\n",
    "    \n",
    "    # Calcular o percentil desejado dos means_norms\n",
    "    percentile = np.percentile([means_norms[i] for i in selected_indices], 7)\n",
    "    \n",
    "    # Selecionar os componentes com means_norms abaixo do percentil e do limite superior\n",
    "    selected_indices_filtered = [i for i in selected_indices if means_norms[i] <= percentile and means_norms[i] <= norm_threshold]\n",
    "    \n",
    "    return selected_indices_filtered\n",
    "\n",
    "# Lista de categorias para iterar\n",
    "categories = ['Bedroom', 'Suburb', 'Industry', 'Kitchen', 'LivingRoom', 'Coast', 'Forest', \n",
    "              'Highway', 'InsideCity', 'Mountain', 'OpenCountry', 'Street', 'Building', \n",
    "              'Office', 'Store']\n",
    "\n",
    "# Trabalhando apenas com o PCA de 95% de vari√¢ncia explicada\n",
    "perc = 95\n",
    "\n",
    "selected_indices_dict = {}\n",
    "\n",
    "# Iterar sobre as categorias para calcular tanto intra-categories quanto cross-categories\n",
    "for category in categories:\n",
    "    for other_category in categories:\n",
    "        # Carregar os componentes e vari√¢ncia explicada da pr√≥pria categoria ou da outra categoria\n",
    "        components = pca_results[perc][other_category]['components']\n",
    "        explained_variance_ratio = pca_results[perc][other_category]['explained_variance_ratio']\n",
    "        \n",
    "        if category == other_category:\n",
    "            print(f\"\\nCategory: {category} on {category} (Intra-Category), Percentage: {perc}%\")\n",
    "        else:\n",
    "            print(f\"\\nCategory: {category} on {other_category} (Cross-Category), Percentage: {perc}%\")\n",
    "        \n",
    "        # Proje√ß√£o dos patches (intra ou cross-categoria)\n",
    "        all_means_norms = []\n",
    "        all_selected_indices = []\n",
    "        \n",
    "        # Iterando pelas imagens e patches\n",
    "        for image_idx in range(centralized_test_features_by_category[category].shape[0]):\n",
    "            patches = centralized_test_features_by_category[category][image_idx]  # Seleciona os patches da imagem\n",
    "\n",
    "            # Projeta os patches nos componentes\n",
    "            projected_patches = project_test_patches(patches, components)\n",
    "            \n",
    "            # Calcular normas, m√©dias e m√©dias das normas para cada imagem\n",
    "            norms_category, means_category, means_norms_category = calculate_norms_and_means(projected_patches, projected_patches)\n",
    "            \n",
    "            # Capturar os componentes cujas m√©dias das normas est√£o abaixo do percentil e explicam at√© ~90% da vari√¢ncia\n",
    "            selected_indices = capture_components_by_percentile_and_threshold(explained_variance_ratio, means_norms_category, exclude_first=True, norm_threshold=50)\n",
    "            \n",
    "            # Armazenar os resultados de m√©dias das normas e componentes selecionados\n",
    "            all_means_norms.append(means_norms_category)\n",
    "            all_selected_indices.append(selected_indices)\n",
    "        \n",
    "        # Verifique se existem componentes selecionados\n",
    "        if len(all_selected_indices) == 0 or np.concatenate(all_selected_indices).size == 0:\n",
    "            print(f\"Warning: No components selected for {category} on {other_category}. Skipping this combination.\")\n",
    "            continue\n",
    "\n",
    "        # Agregue os componentes selecionados em todas as imagens\n",
    "        aggregated_selected_indices = np.unique(np.concatenate(all_selected_indices)).astype(int)  # Convers√£o para inteiros\n",
    "        \n",
    "        # Inicializar os dicion√°rios se as chaves n√£o existirem\n",
    "        if category not in selected_indices_dict:\n",
    "            selected_indices_dict[category] = {}\n",
    "        \n",
    "        selected_indices_dict[category][other_category] = aggregated_selected_indices\n",
    "\n",
    "        # Evite plotagens se n√£o houver componentes selecionados\n",
    "        if len(aggregated_selected_indices) == 0:\n",
    "            print(f\"Warning: No valid components selected for {category} on {other_category}. Skipping plot.\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def project_and_transform_back(features, pca, specific_indices):\n",
    "    \"\"\"\n",
    "    Projeta as features nos componentes principais espec√≠ficos e reconstr√≥i a partir desses componentes.\n",
    "    \"\"\"\n",
    "    # Proje√ß√£o das features nos componentes principais\n",
    "    projected = pca.transform(features)\n",
    "    \n",
    "    # Usar apenas os componentes espec√≠ficos\n",
    "    projected_specific = projected[:, specific_indices]\n",
    "    \n",
    "    # Reconstruir as features apenas com os componentes espec√≠ficos\n",
    "    specific_components = pca.components_[specific_indices]\n",
    "    reconstructed_features = np.dot(projected_specific, specific_components)\n",
    "    \n",
    "    return reconstructed_features\n",
    "\n",
    "def calculate_mean_ood_for_specific_components(original_features, pca, specific_indices):\n",
    "    \"\"\"\n",
    "    Projeta as features originais em componentes PCA espec√≠ficos, reconstr√≥i e calcula a m√©dia dos OOD scores.\n",
    "    \"\"\"\n",
    "    total_ood_scores = []\n",
    "    \n",
    "    # Itera sobre todas as amostras de features\n",
    "    for sample_idx in range(original_features.shape[0]):\n",
    "        features = original_features[sample_idx]\n",
    "        \n",
    "        # Proje√ß√£o e reconstru√ß√£o das features nos componentes espec√≠ficos\n",
    "        reconstructed_features = project_and_transform_back(features.reshape(1, -1), pca, specific_indices)\n",
    "        \n",
    "        # Calcula os res√≠duos (erro de reconstru√ß√£o)\n",
    "        residuals = features - reconstructed_features.flatten()\n",
    "        \n",
    "        # Calcular a norma das features originais e dos res√≠duos\n",
    "        original_norm = np.linalg.norm(features)\n",
    "        residual_norm = np.linalg.norm(residuals)\n",
    "        \n",
    "        # Verifique se a norma dos res√≠duos √© maior que a norma das features originais\n",
    "        if residual_norm > original_norm:\n",
    "            print(f\"Warning: Residual norm ({residual_norm}) greater than original norm ({original_norm}) for sample {sample_idx}\")\n",
    "        \n",
    "        # Calcular a pontua√ß√£o OOD (norma dos res√≠duos sobre a norma das features originais)\n",
    "        if original_norm == 0:\n",
    "            ood_score = 0\n",
    "        else:\n",
    "            ood_score = residual_norm / original_norm\n",
    "        \n",
    "        # Adiciona a pontua√ß√£o OOD desta amostra √† lista total\n",
    "        total_ood_scores.append(ood_score)\n",
    "    \n",
    "    # Retorna a m√©dia das pontua√ß√µes OOD\n",
    "    return np.mean(total_ood_scores)\n",
    "\n",
    "# Iterar sobre as categorias para calcular as m√©dias das pontua√ß√µes OOD\n",
    "mean_ood_scores = {}\n",
    "\n",
    "for category in categories:\n",
    "    for other_category in categories:\n",
    "        specific_indices = selected_indices_dict[category][other_category]\n",
    "        \n",
    "        # Recupera os objetos PCA para as categorias correspondentes\n",
    "        pca_object = pca_results[perc][other_category]['pca_object']  # Usamos os componentes do other_category\n",
    "        \n",
    "        # Verificar se as features de teste existem para a categoria\n",
    "        if category not in centralized_test_features_by_category:\n",
    "            print(f\"Warning: No test features found for {category}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Calcular a m√©dia das pontua√ß√µes OOD com base na proje√ß√£o nos componentes espec√≠ficos\n",
    "        mean_ood = calculate_mean_ood_for_specific_components(centralized_test_features_by_category[category], pca_object, specific_indices)\n",
    "        \n",
    "        # Armazenar a m√©dia no dicion√°rio\n",
    "        mean_ood_scores[f\"{category}_on_{other_category}\"] = mean_ood\n",
    "\n",
    "# Exibir todas as m√©dias calculadas\n",
    "for key, mean_ood in mean_ood_scores.items():\n",
    "    print(f\"Mean OOD Score for {key}: {mean_ood}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Lista de categorias dispon√≠veis\n",
    "categories = ['Bedroom', 'Suburb', 'Industry', 'Kitchen', 'LivingRoom', 'Coast', 'Forest', \n",
    "              'Highway', 'InsideCity', 'Mountain', 'OpenCountry', 'Street', 'Building', \n",
    "              'Office', 'Store']\n",
    "\n",
    "# Inicializar uma matriz vazia para armazenar os OOD scores\n",
    "ood_score_matrix = np.full((len(categories), len(categories)), np.nan)\n",
    "\n",
    "# Preencher a matriz com os OOD scores calculados\n",
    "for i, test_category in enumerate(categories):\n",
    "    for j, train_category in enumerate(categories):\n",
    "        key = f\"{test_category}_on_{train_category}\"\n",
    "        if key in mean_ood_scores:\n",
    "            ood_score_matrix[i, j] = mean_ood_scores[key]\n",
    "\n",
    "# Criar um DataFrame a partir da matriz de OOD scores para facilitar o plot\n",
    "ood_score_df = pd.DataFrame(ood_score_matrix, index=categories, columns=categories)\n",
    "\n",
    "# Plotar o heatmap usando apenas matplotlib\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Criar o heatmap com imshow\n",
    "cax = ax.imshow(ood_score_df, cmap=\"coolwarm\", aspect=\"auto\")\n",
    "\n",
    "# Adicionar os valores na matriz\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(categories)):\n",
    "        value = ood_score_matrix[i, j]\n",
    "        if not np.isnan(value):\n",
    "            ax.text(j, i, f'{value:.4f}', ha='center', va='center', color='black')\n",
    "\n",
    "# Configurar os eixos\n",
    "ax.set_xticks(np.arange(len(categories)))\n",
    "ax.set_yticks(np.arange(len(categories)))\n",
    "ax.set_xticklabels(categories, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(categories)\n",
    "\n",
    "# Adicionar t√≠tulo e r√≥tulos dos eixos\n",
    "ax.set_title('Heatmap of OOD Scores for Test and Train Categories (Specific PCA Components)')\n",
    "ax.set_xlabel('Train Category')\n",
    "ax.set_ylabel('Test Category')\n",
    "\n",
    "# Adicionar a barra de cores (colorbar)\n",
    "fig.colorbar(cax, ax=ax, label='OOD Score')\n",
    "\n",
    "# Ajustar layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar o heatmap\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
