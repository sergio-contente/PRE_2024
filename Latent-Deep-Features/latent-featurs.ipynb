{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ Out-of-Distribution (OOD) with PCA using Deep Features from the Latent Space\n",
    "\n",
    "The goal of this notebook is to understand the depths of using Principal Component Analysis in order to perform OOD tasks using deep features from the latent space\n",
    "\n",
    "## üìù Plan of action\n",
    "\n",
    "### ‚ôªÔ∏è Preprocessing phase\n",
    "\n",
    "In order to achieve our goal, we need to understand how the dataset is structured.\n",
    "\n",
    "For this notebook, we are going to use the CBIR 15 dataset, that contains images of different places, such as an office, a bedroom, a mountain, etc. Note that there are some places that are similar one to another, i.e. a bedroom and a living room.\n",
    "\n",
    "Thus, in order to extract the features of the images we have to preprocess those images:\n",
    "\n",
    "- Get the images that are located in data/CBIR_15-scene and fit them to a dataframe using Pandas\n",
    "  - Locate the \"Labels.txt\" file: it shows where the indexes of the images from each category starts\n",
    "- Create the dataset with this information with two columns: the path to the image and its category\n",
    "- Transform all of the images in the same size (in this case, we are going with 256x256)\n",
    "  \n",
    "Now, in order to extract the features, it's necessary to divide the reshaped images into patches of 32x32 pixels. This is good to perform processing tasks to avoid waiting long periods of time.\n",
    "\n",
    "After all the preprocess, we should separate the images into two different foldes: one contains the patches of the training images that is going to give us their principal components and dimensions, and the other is the patches of the test images, that is going to be tested to fit into those dimensions and we'll get an OOD score afterwards.\n",
    "\n",
    "### üèãüèΩ‚Äç‚ôÇÔ∏è Training phase\n",
    "\n",
    "With the images that are stored inside the \"patches_train\" folder, the first thing we are going to do is _normalize_ all of the images to find the correct maximum covariance and transforming all the variables into the same scale.\n",
    "\n",
    "Next, we should then apply the PCA with all the components. As we have patches of 32x32, we'll be having 1024 features, hence components. Then we plot a graph to see how many components truly contributes for the most variance of the data - and give us more information about it. We're going to take the threshold of 95% of variance in this notebook.\n",
    "\n",
    "After getting the PCA with components that describe 95% of the variance, it's time to test our images and see how far of the residual space their data can be found.\n",
    "\n",
    "### ‚öóÔ∏è Test phase and results\n",
    "\n",
    "In this phase, we take the test images and normalize then with the same scale of each PCA. This is important to maintain consistency throughout the final results and measure the norms in the new dimension properly.\n",
    "\n",
    "After that, we calculate the norm of the projection of the given data into the orthogonal space of the principal component and divide it by the norm of the data in relation to the origin. This is the OOD score.\n",
    "\n",
    "We calculate the mean of the score for each category and get the minimal one. The current environment is the smallest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "First of all, we need to understand which libraries we are going to use:\n",
    "\n",
    "- os: Deals with the operation system interface such as finding the relative and absolute path of files inside a project and reading/writing files for example.\n",
    "- sys: This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter.\n",
    "- numpy: NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\n",
    "- pandas: Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- matplotlib: Deals with plotting graphs to visualize data in a graphical way.\n",
    "- sklearn: Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd suggest to use a conda virtual environment in order to avoid messing up your base kernel environment and causing dependency errors in the future.\n",
    "\n",
    "After you successfully installed all the modules, it's time to import our custom modules that are going to deal with:\n",
    "\n",
    "- Creation of our dataframe using pandas\n",
    "- Separation of our dataset into patches of 32x32 in folders of training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from dataframe_generator import *\n",
    "from images_standardizing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14046/1998898793.py:8: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=extract_to)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos extra√≠dos para ../data/\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_tgz(tgz_path, extract_to):\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "    \n",
    "    with tarfile.open(tgz_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_to)\n",
    "        print(f\"Arquivos extra√≠dos para {extract_to}\")\n",
    "\n",
    "tgz_path = '../CBIR_15-Scene.tgz'\n",
    "extract_to = '../data/'\n",
    "\n",
    "extract_tgz(tgz_path, extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             image_path category\n",
      "0        ../data/CBIR_15-Scene/00/1.jpg  Bedroom\n",
      "1        ../data/CBIR_15-Scene/00/2.jpg  Bedroom\n",
      "2        ../data/CBIR_15-Scene/00/3.jpg  Bedroom\n",
      "3        ../data/CBIR_15-Scene/00/4.jpg  Bedroom\n",
      "4        ../data/CBIR_15-Scene/00/5.jpg  Bedroom\n",
      "...                                 ...      ...\n",
      "4480  ../data/CBIR_15-Scene/14/4481.jpg    Store\n",
      "4481  ../data/CBIR_15-Scene/14/4482.jpg    Store\n",
      "4482  ../data/CBIR_15-Scene/14/4483.jpg    Store\n",
      "4483  ../data/CBIR_15-Scene/14/4484.jpg    Store\n",
      "4484  ../data/CBIR_15-Scene/14/4485.jpg    Store\n",
      "\n",
      "[4485 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/CBIR_15-Scene/00/1.jpg</td>\n",
       "      <td>Bedroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/CBIR_15-Scene/00/2.jpg</td>\n",
       "      <td>Bedroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/CBIR_15-Scene/00/3.jpg</td>\n",
       "      <td>Bedroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/CBIR_15-Scene/00/4.jpg</td>\n",
       "      <td>Bedroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/CBIR_15-Scene/00/5.jpg</td>\n",
       "      <td>Bedroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>../data/CBIR_15-Scene/14/4481.jpg</td>\n",
       "      <td>Store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>../data/CBIR_15-Scene/14/4482.jpg</td>\n",
       "      <td>Store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4482</th>\n",
       "      <td>../data/CBIR_15-Scene/14/4483.jpg</td>\n",
       "      <td>Store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>../data/CBIR_15-Scene/14/4484.jpg</td>\n",
       "      <td>Store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4484</th>\n",
       "      <td>../data/CBIR_15-Scene/14/4485.jpg</td>\n",
       "      <td>Store</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4485 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image_path category\n",
       "0        ../data/CBIR_15-Scene/00/1.jpg  Bedroom\n",
       "1        ../data/CBIR_15-Scene/00/2.jpg  Bedroom\n",
       "2        ../data/CBIR_15-Scene/00/3.jpg  Bedroom\n",
       "3        ../data/CBIR_15-Scene/00/4.jpg  Bedroom\n",
       "4        ../data/CBIR_15-Scene/00/5.jpg  Bedroom\n",
       "...                                 ...      ...\n",
       "4480  ../data/CBIR_15-Scene/14/4481.jpg    Store\n",
       "4481  ../data/CBIR_15-Scene/14/4482.jpg    Store\n",
       "4482  ../data/CBIR_15-Scene/14/4483.jpg    Store\n",
       "4483  ../data/CBIR_15-Scene/14/4484.jpg    Store\n",
       "4484  ../data/CBIR_15-Scene/14/4485.jpg    Store\n",
       "\n",
       "[4485 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òùÔ∏è Part I: Comparing two different environments\n",
    "\n",
    "### ‚ôªÔ∏è Preprocessing phase\n",
    "\n",
    "Now we start our experiments to understand if our idea work, however this time we are going to understand what happens with our approach using two different environments.\n",
    "\n",
    "In our case, I'm going to take the **Coast** and **Office** environments arbitrarily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>../data/CBIR_15-Scene/05/1268.jpg</td>\n",
       "      <td>Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>../data/CBIR_15-Scene/05/1269.jpg</td>\n",
       "      <td>Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>../data/CBIR_15-Scene/05/1270.jpg</td>\n",
       "      <td>Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>../data/CBIR_15-Scene/05/1271.jpg</td>\n",
       "      <td>Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>../data/CBIR_15-Scene/05/1272.jpg</td>\n",
       "      <td>Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4165</th>\n",
       "      <td>../data/CBIR_15-Scene/13/4166.jpg</td>\n",
       "      <td>Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4166</th>\n",
       "      <td>../data/CBIR_15-Scene/13/4167.jpg</td>\n",
       "      <td>Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4167</th>\n",
       "      <td>../data/CBIR_15-Scene/13/4168.jpg</td>\n",
       "      <td>Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4168</th>\n",
       "      <td>../data/CBIR_15-Scene/13/4169.jpg</td>\n",
       "      <td>Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4169</th>\n",
       "      <td>../data/CBIR_15-Scene/13/4170.jpg</td>\n",
       "      <td>Office</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>575 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image_path category\n",
       "1267  ../data/CBIR_15-Scene/05/1268.jpg    Coast\n",
       "1268  ../data/CBIR_15-Scene/05/1269.jpg    Coast\n",
       "1269  ../data/CBIR_15-Scene/05/1270.jpg    Coast\n",
       "1270  ../data/CBIR_15-Scene/05/1271.jpg    Coast\n",
       "1271  ../data/CBIR_15-Scene/05/1272.jpg    Coast\n",
       "...                                 ...      ...\n",
       "4165  ../data/CBIR_15-Scene/13/4166.jpg   Office\n",
       "4166  ../data/CBIR_15-Scene/13/4167.jpg   Office\n",
       "4167  ../data/CBIR_15-Scene/13/4168.jpg   Office\n",
       "4168  ../data/CBIR_15-Scene/13/4169.jpg   Office\n",
       "4169  ../data/CBIR_15-Scene/13/4170.jpg   Office\n",
       "\n",
       "[575 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_categories = ['Coast', 'Office']\n",
    "\n",
    "df_different = df[df['category'].isin(train_categories)]\n",
    "df_different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to separate our dataset into train and test. We should use the built-in function of sklearn to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories: ['Coast', 'Office']\n"
     ]
    }
   ],
   "source": [
    "X = df_different['image_path'].tolist()\n",
    "y = df_different['category'].tolist()\n",
    "unique_categories = list(df_different['category'].unique())\n",
    "print(f\"Unique categories: {unique_categories}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "standard_size = (256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that everything went well, we plot the grid of all the patches from the first image of our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what the module that's inside our \"image_patching.py\" do. So we now, need to save everything into the subfolders by calling that function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_images_set(X_train, X_test, y_train, y_test, output_dir_train='images_train', output_dir_test='images_test', standard_size=standard_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should load our patches for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "training_images_by_category = load_images_by_category('images_train', unique_categories, image_size=(256, 256))\n",
    "print(training_images_by_category['Coast'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering images\n",
    "\n",
    "Now, we need to center the images to make the neural network more efficient. We are not normalizing it to avoid information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348, 256, 256)\n",
      "Category Coast, images shape: (348, 256, 256)\n",
      "(209, 256, 256)\n",
      "Category Office, images shape: (209, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "def center_images(images):\n",
    "    num_images, height, width = images.shape\n",
    "    flattened_images = images.reshape((num_images, -1))\n",
    "    \n",
    "    mean = np.mean(flattened_images, axis=0)\n",
    "    \n",
    "    centered_flattened_images = flattened_images - mean\n",
    "    \n",
    "    centered_images = centered_flattened_images.reshape((num_images, height, width))\n",
    "    return centered_images\n",
    "\n",
    "\n",
    "centralized_images_by_category = {}\n",
    "scalers_by_category = {}\n",
    "for category, images in training_images_by_category.items():\n",
    "    print(images.shape)\n",
    "    centralized_images = center_images(images)\n",
    "    centralized_images_by_category[category] = centralized_images\n",
    "    print(f\"Category {category}, images shape: {centralized_images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean pixel values after centralization for category Coast: -1.013238024772844e-15\n",
      "Mean pixel values after centralization for category Office: 5.664793461532378e-15\n"
     ]
    }
   ],
   "source": [
    "def check_centralization(images):\n",
    "    mean = np.mean(images, axis=(0, 1, 2))\n",
    "    return mean\n",
    "\n",
    "for category, images in centralized_images_by_category.items():\n",
    "    mean = check_centralization(images)\n",
    "    print(f\"Mean pixel values after centralization for category {category}: {mean}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the values close to zero, it means that the pixels for each color channel are correctly centralized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãüèΩ‚Äç‚ôÇÔ∏è Training phase\n",
    "\n",
    "With everything preprocessed, we now need to train our neural network. In this notebook, I chose the VGG16 because it's a well-known neural network that is often used por computer vision applications.\n",
    "\n",
    "I'm using no weights, because the underlining goal of this research is to use the results from this work in a unsupervised environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get the before last layer's output to extract our latent features from the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result means that we extracted 348 images with 4096 features each of the Coast category and 209 images with 4096 features each of the Office category.\n",
    "\n",
    "Now we have to reduce the dimensonality. In order to do that, we should use PCA techniques. But before that, we should centralize the features now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/contente/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/contente/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.0937\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Treinando o modelo U-Net por 10 √©pocas (ou ajuste conforme necess√°rio)\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mtrain_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Visualizando as primeiras imagens de cada categoria\u001b[39;00m\n\u001b[1;32m    163\u001b[0m unet\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[11], line 135\u001b[0m, in \u001b[0;36mtrain_unet\u001b[0;34m(unet, data_loader, num_epochs)\u001b[0m\n\u001b[1;32m    132\u001b[0m images \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m    134\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 135\u001b[0m reconstructed_images, _ \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed_images, images)\n\u001b[1;32m    137\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 83\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Encoder path\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     enc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     enc2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder2(F\u001b[38;5;241m.\u001b[39mmax_pool2d(enc1, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     85\u001b[0m     enc3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder3(F\u001b[38;5;241m.\u001b[39mmax_pool2d(enc2, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:176\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-dl/lib/python3.12/site-packages/torch/nn/functional.py:2512\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2510\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2514\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Clear GPU cache before running the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# UNet model modified to output features\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = self.contracting_block(in_channels, 64)\n",
    "        self.encoder2 = self.contracting_block(64, 128)\n",
    "        self.encoder3 = self.contracting_block(128, 256)\n",
    "        self.encoder4 = self.contracting_block(256, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self.expansive_block(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.expansive_block(256, 128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.expansive_block(128, 64)\n",
    "        self.decoder1 = self.final_block(64, out_channels)\n",
    "\n",
    "    def contracting_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(kernel_size=3, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_size=3, in_channels=out_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def expansive_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(kernel_size=3, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_size=3, in_channels=out_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def final_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(kernel_size=3, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_size=3, in_channels=out_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_size=1, in_channels=out_channels, out_channels=out_channels),\n",
    "            nn.Sigmoid()  # Adiciona ativa√ß√£o sigmoid para garantir que a sa√≠da fique entre [0, 1]\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        _, _, H, W = upsampled.size()\n",
    "        _, _, H_b, W_b = bypass.size()\n",
    "        if H_b != H or W_b != W:\n",
    "            bypass = F.interpolate(bypass, size=(H, W), mode='bilinear', align_corners=True)\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(F.max_pool2d(enc1, kernel_size=2, stride=2))\n",
    "        enc3 = self.encoder3(F.max_pool2d(enc2, kernel_size=2, stride=2))\n",
    "        enc4 = self.encoder4(F.max_pool2d(enc3, kernel_size=2, stride=2))\n",
    "\n",
    "        # Decoder path\n",
    "        dec4 = self.crop_and_concat(self.upconv4(enc4), enc3)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.crop_and_concat(self.upconv3(dec4), enc2)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.crop_and_concat(self.upconv2(dec3), enc1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.decoder1(dec2)\n",
    "\n",
    "        return dec1, enc4  # Retorna tanto a imagem reconstru√≠da quanto as features da pen√∫ltima camada\n",
    "\n",
    "# VGG-based perceptual loss (Feature Extractor)\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.slice1 = nn.Sequential(*vgg[:5])   # conv1_2\n",
    "        self.slice2 = nn.Sequential(*vgg[5:10]) # conv2_2\n",
    "        self.slice3 = nn.Sequential(*vgg[10:19])# conv3_4\n",
    "        self.slice4 = nn.Sequential(*vgg[19:28])# conv4_4\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Converte imagens grayscale para 3 canais duplicando o √∫nico canal\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "        h_relu1 = self.slice1(x)\n",
    "        h_relu2 = self.slice2(h_relu1)\n",
    "        h_relu3 = self.slice3(h_relu2)\n",
    "        h_relu4 = self.slice4(h_relu3)\n",
    "        return [h_relu1, h_relu2, h_relu3, h_relu4]\n",
    "\n",
    "# Fun√ß√£o de treinamento do modelo\n",
    "def train_unet(unet, data_loader, num_epochs=20):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(unet.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        unet.train()\n",
    "        epoch_loss = 0\n",
    "        for images, in data_loader:\n",
    "            images = images.to(device).float()\n",
    "            \n",
    "            # Normaliza√ß√£o para intervalo [0, 1]\n",
    "            images /= 255.0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_images, _ = unet(images)\n",
    "            loss = criterion(reconstructed_images, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(data_loader):.4f}')\n",
    "\n",
    "# Inicializa√ß√£o dos modelos UNet e VGG para Perceptual Loss\n",
    "unet = UNet().to(device)\n",
    "vgg_loss = VGGPerceptualLoss().to(device)\n",
    "\n",
    "# Defina o DataLoader aqui\n",
    "categories = centralized_images_by_category.keys()\n",
    "\n",
    "for category in categories:\n",
    "    images = centralized_images_by_category[category]\n",
    "\n",
    "    # Adiciona a dimens√£o do canal e cria o DataLoader\n",
    "    images = np.expand_dims(images, axis=1)  # Adiciona dimens√£o do canal\n",
    "    dataset = TensorDataset(torch.tensor(images, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Treinando o modelo U-Net por 10 √©pocas (ou ajuste conforme necess√°rio)\n",
    "    train_unet(unet, loader, num_epochs=20)\n",
    "\n",
    "    # Visualizando as primeiras imagens de cada categoria\n",
    "    unet.eval()\n",
    "    vgg_loss.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(3):  # Exibe 3 imagens de cada categoria\n",
    "            image = centralized_images_by_category[category][i]\n",
    "            \n",
    "            # Normaliza√ß√£o da imagem antes de passar pela UNet\n",
    "            image_tensor = torch.tensor(image / 255.0).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "            # Processa com UNet\n",
    "            reconstructed_image, unet_features = unet(image_tensor)\n",
    "\n",
    "            # Extrai features com o modelo VGG\n",
    "            vgg_features = vgg_loss(image_tensor)\n",
    "            vgg_features_combined = torch.cat([f.view(-1) for f in vgg_features], dim=0).cpu().numpy()\n",
    "\n",
    "            # Extrai as features da U-Net e as converte para numpy\n",
    "            unet_features_combined = unet_features.view(-1).cpu().numpy()\n",
    "\n",
    "            # Converte o tensor para numpy para visualiza√ß√£o\n",
    "            reconstructed_image_np = reconstructed_image.squeeze().cpu().numpy()\n",
    "\n",
    "            # Visualiza as imagens\n",
    "            plt.figure(figsize=(20, 5))\n",
    "            \n",
    "            # Imagem original\n",
    "            plt.subplot(1, 4, 1)\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.title(f'Original {category} Image {i+1}')\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Imagem reconstru√≠da\n",
    "            plt.subplot(1, 4, 2)\n",
    "            plt.imshow(reconstructed_image_np, cmap='gray')\n",
    "            plt.title(f'Reconstructed {category} Image {i+1}')\n",
    "            plt.axis('off')\n",
    "\n",
    "            # UNet Features\n",
    "            plt.subplot(1, 4, 3)\n",
    "            plt.plot(unet_features_combined)\n",
    "            plt.title(f'UNet Features {category} Image {i+1}')\n",
    "            plt.axis('on')\n",
    "\n",
    "            # VGG Features\n",
    "            plt.subplot(1, 4, 4)\n",
    "            plt.plot(vgg_features_combined)\n",
    "            plt.title(f'VGG Features {category} Image {i+1}')\n",
    "            plt.axis('on')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torchvision import models\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Verifica o dispositivo dispon√≠vel (GPU ou CPU)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f'Using device: {device}')\n",
    "\n",
    "# # Define o modelo de autoencoder convolucional simples\n",
    "# class ConvAutoencoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ConvAutoencoder, self).__init__()\n",
    "#         # Encoder\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),  # [B, 64, 112, 112]\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # [B, 128, 56, 56]\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # [B, 256, 28, 28]\n",
    "#             nn.ReLU(True)\n",
    "#         )\n",
    "#         # Decoder\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # [B, 128, 56, 56]\n",
    "#             nn.ReLU(True),\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # [B, 64, 112, 112]\n",
    "#             nn.ReLU(True),\n",
    "#             nn.ConvTranspose2d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # [B, 1, 224, 224]\n",
    "#             nn.Sigmoid()  # Sa√≠da no intervalo [0, 1]\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         x = self.decoder(x)\n",
    "#         return x\n",
    "\n",
    "# # Define a classe de VGG Perceptual Loss\n",
    "# class VGGPerceptualLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VGGPerceptualLoss, self).__init__()\n",
    "#         vgg = models.vgg19(pretrained=True).features\n",
    "#         self.slice1 = nn.Sequential(*vgg[:5])   # Conv1_2\n",
    "#         self.slice2 = nn.Sequential(*vgg[5:10]) # Conv2_2\n",
    "#         self.slice3 = nn.Sequential(*vgg[10:19])# Conv3_4\n",
    "#         self.slice4 = nn.Sequential(*vgg[19:28])# Conv4_4\n",
    "#         for p in self.parameters():\n",
    "#             p.requires_grad = False\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Converter para 3 canais duplicando o √∫nico canal (no caso de grayscale)\n",
    "#         x = x.repeat(1, 3, 1, 1)\n",
    "#         h_relu1 = self.slice1(x)\n",
    "#         h_relu2 = self.slice2(h_relu1)\n",
    "#         h_relu3 = self.slice3(h_relu2)\n",
    "#         h_relu4 = self.slice4(h_relu3)\n",
    "#         return [h_relu1, h_relu2, h_relu3, h_relu4]\n",
    "\n",
    "# # Fun√ß√£o de perda perceptual combinada com MSE\n",
    "# def perceptual_loss_function(vgg_loss, recon_images, orig_images):\n",
    "#     # Extrai as caracter√≠sticas da imagem original e reconstru√≠da\n",
    "#     orig_features = vgg_loss(orig_images)\n",
    "#     recon_features = vgg_loss(recon_images)\n",
    "    \n",
    "#     # Calcula a perda perceptual como MSE das caracter√≠sticas intermedi√°rias\n",
    "#     perceptual_loss = 0\n",
    "#     for orig_f, recon_f in zip(orig_features, recon_features):\n",
    "#         perceptual_loss += nn.functional.mse_loss(recon_f, orig_f)\n",
    "    \n",
    "#     return perceptual_loss\n",
    "\n",
    "# # Fun√ß√£o de treinamento\n",
    "# def train_autoencoder(autoencoder, vgg_loss, data_loader, num_epochs=20):\n",
    "#     # Usa Adam como otimizador\n",
    "#     optimizer = optim.Adam(autoencoder.parameters(), lr=0.0001)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         autoencoder.train()\n",
    "#         epoch_loss = 0\n",
    "        \n",
    "#         for images, in data_loader:\n",
    "#             images = images.to(device).float()\n",
    "#             images /= 255.0  # Normaliza para [0, 1]\n",
    "\n",
    "#             # Passa as imagens pelo autoencoder\n",
    "#             recon_images = autoencoder(images)\n",
    "            \n",
    "#             # Calcula a perda perceptual\n",
    "#             perceptual_loss = perceptual_loss_function(vgg_loss, recon_images, images)\n",
    "#             mse_loss = nn.functional.mse_loss(recon_images, images)\n",
    "#             total_loss = perceptual_loss + mse_loss\n",
    "\n",
    "#             # Otimiza o modelo\n",
    "#             optimizer.zero_grad()\n",
    "#             total_loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             epoch_loss += total_loss.item()\n",
    "\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(data_loader):.4f}')\n",
    "\n",
    "# # Inicializa o autoencoder e a VGG perceptual loss\n",
    "# autoencoder = ConvAutoencoder().to(device)\n",
    "# vgg_loss = VGGPerceptualLoss().to(device)\n",
    "\n",
    "# # Defina o DataLoader aqui\n",
    "# categories = centralized_images_by_category.keys()\n",
    "\n",
    "# for category in categories:\n",
    "#     images = centralized_images_by_category[category]\n",
    "\n",
    "#     # Adiciona a dimens√£o do canal e cria o DataLoader\n",
    "#     images = np.expand_dims(images, axis=1)  # Adiciona dimens√£o do canal\n",
    "#     dataset = TensorDataset(torch.tensor(images, dtype=torch.float32))\n",
    "#     loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "#     # Treina o modelo de autoencoder\n",
    "#     train_autoencoder(autoencoder, vgg_loss, loader, num_epochs=20)\n",
    "\n",
    "#     # Avalia e visualiza algumas imagens reconstru√≠das\n",
    "#     autoencoder.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(3):  # Exibe 3 imagens de cada categoria\n",
    "#             image = centralized_images_by_category[category][i]\n",
    "#             image_tensor = torch.tensor(image / 255.0).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "#             # Reconstr√≥i a imagem\n",
    "#             reconstructed_image = autoencoder(image_tensor)\n",
    "\n",
    "#             # Converte para numpy para visualiza√ß√£o\n",
    "#             reconstructed_image_np = reconstructed_image.squeeze().cpu().numpy()\n",
    "\n",
    "#             # Visualiza as imagens\n",
    "#             plt.figure(figsize=(10, 5))\n",
    "#             plt.subplot(1, 2, 1)\n",
    "#             plt.imshow(image, cmap='gray')\n",
    "#             plt.title(f'Original {category} Image {i+1}')\n",
    "#             plt.axis('off')\n",
    "\n",
    "#             plt.subplot(1, 2, 2)\n",
    "#             plt.imshow(reconstructed_image_np, cmap='gray')\n",
    "#             plt.title(f'Reconstructed {category} Image {i+1}')\n",
    "#             plt.axis('off')\n",
    "\n",
    "#             plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Fun√ß√£o para aplicar PCA reduzida\n",
    "def apply_reduced_pca(features_by_category, n_components=1024, number_variance=0.95):\n",
    "    pca_by_category = {}\n",
    "    num_components_reduced_dict = {}\n",
    "    \n",
    "    for category, features in features_by_category.items():\n",
    "        all_features = np.vstack(features)  # Agrupa todas as features da categoria em uma matriz\n",
    "        \n",
    "        # Verifica se h√° features para processar\n",
    "        if all_features.size == 0:\n",
    "            continue\n",
    "        \n",
    "        # Ajusta dinamicamente o n√∫mero de componentes baseado em min(n_samples, n_features)\n",
    "        n_samples, n_features = all_features.shape\n",
    "        max_components = min(n_samples, n_features)\n",
    "\n",
    "        # Ajusta a PCA com o n√∫mero inicial de componentes\n",
    "        pca = PCA(n_components=max_components)\n",
    "        pca.fit(all_features)\n",
    "        \n",
    "        # Determina o n√∫mero de componentes que ret√™m a vari√¢ncia desejada\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        num_components_reduced = np.where(cumulative_variance >= number_variance)[0][0] + 1\n",
    "        \n",
    "        # Reajusta a PCA para o n√∫mero reduzido de componentes\n",
    "        pca = PCA(n_components=num_components_reduced)\n",
    "        pca.fit(all_features)\n",
    "        \n",
    "        # Armazena a PCA e o n√∫mero de componentes reduzidos por categoria\n",
    "        pca_by_category[category] = pca\n",
    "        num_components_reduced_dict[category] = num_components_reduced\n",
    "\n",
    "    # Calcula o n√∫mero m√≠nimo de componentes entre todas as categorias\n",
    "    min_num_components = min(num_components_reduced_dict.values())\n",
    "    return pca_by_category, num_components_reduced_dict, min_num_components\n",
    "\n",
    "\n",
    "# Extrair features por categoria para UNet e VGG\n",
    "def extract_features_by_category(unet, vgg_loss, images_by_category):\n",
    "    unet_features_by_category = {}\n",
    "    vgg_features_by_category = {}\n",
    "\n",
    "    for category, images in images_by_category.items():\n",
    "        unet_features_by_category[category] = []\n",
    "        vgg_features_by_category[category] = []\n",
    "        \n",
    "        for image in images:\n",
    "            image_tensor = torch.tensor(image / 255.0).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "            # Extrai as features da U-Net\n",
    "            _, unet_features = unet(image_tensor)\n",
    "            unet_features_combined = unet_features.view(-1).detach().cpu().numpy()\n",
    "            unet_features_by_category[category].append(unet_features_combined)\n",
    "\n",
    "            # Extrai as features da VGG\n",
    "            vgg_features = vgg_loss(image_tensor)\n",
    "            vgg_features_combined = torch.cat([f.view(-1).detach() for f in vgg_features], dim=0).cpu().numpy()\n",
    "            vgg_features_by_category[category].append(vgg_features_combined)\n",
    "    \n",
    "    return unet_features_by_category, vgg_features_by_category\n",
    "\n",
    "\n",
    "# Fun√ß√£o principal para calcular a PCA reduzida para U-Net e VGG\n",
    "def calculate_pca_for_unet_and_vgg(unet, vgg_loss, images_by_category):\n",
    "    # Extrai as features da U-Net e do VGG por categoria\n",
    "    unet_features_by_category, vgg_features_by_category = extract_features_by_category(unet, vgg_loss, images_by_category)\n",
    "\n",
    "    # Aplica PCA reduzida para as features da U-Net e VGG com vari√¢ncia de 95%, 90%, 85%, e 80%\n",
    "    pca_unet_95, num_components_unet_95, _ = apply_reduced_pca(unet_features_by_category, number_variance=0.95)\n",
    "    pca_vgg_95, num_components_vgg_95, _ = apply_reduced_pca(vgg_features_by_category, number_variance=0.95)\n",
    "\n",
    "    pca_unet_90, num_components_unet_90, _ = apply_reduced_pca(unet_features_by_category, number_variance=0.90)\n",
    "    pca_vgg_90, num_components_vgg_90, _ = apply_reduced_pca(vgg_features_by_category, number_variance=0.90)\n",
    "\n",
    "    pca_unet_85, num_components_unet_85, _ = apply_reduced_pca(unet_features_by_category, number_variance=0.85)\n",
    "    pca_vgg_85, num_components_vgg_85, _ = apply_reduced_pca(vgg_features_by_category, number_variance=0.85)\n",
    "\n",
    "    pca_unet_80, num_components_unet_80, _ = apply_reduced_pca(unet_features_by_category, number_variance=0.80)\n",
    "    pca_vgg_80, num_components_vgg_80, _ = apply_reduced_pca(vgg_features_by_category, number_variance=0.80)\n",
    "\n",
    "    # Retorna as PCAs calculadas para U-Net e VGG\n",
    "    return {\n",
    "        \"unet\": {\n",
    "            \"95_variance\": pca_unet_95,\n",
    "            \"90_variance\": pca_unet_90,\n",
    "            \"85_variance\": pca_unet_85,\n",
    "            \"80_variance\": pca_unet_80\n",
    "        },\n",
    "        \"vgg\": {\n",
    "            \"95_variance\": pca_vgg_95,\n",
    "            \"90_variance\": pca_vgg_90,\n",
    "            \"85_variance\": pca_vgg_85,\n",
    "            \"80_variance\": pca_vgg_80\n",
    "        },\n",
    "        \"num_components\": {\n",
    "            \"unet\": {\n",
    "                \"95_variance\": num_components_unet_95,\n",
    "                \"90_variance\": num_components_unet_90,\n",
    "                \"85_variance\": num_components_unet_85,\n",
    "                \"80_variance\": num_components_unet_80\n",
    "            },\n",
    "            \"vgg\": {\n",
    "                \"95_variance\": num_components_vgg_95,\n",
    "                \"90_variance\": num_components_vgg_90,\n",
    "                \"85_variance\": num_components_vgg_85,\n",
    "                \"80_variance\": num_components_vgg_80\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Aplicando o PCA nas features da U-Net e VGG\n",
    "pca_results = calculate_pca_for_unet_and_vgg(unet, vgg_loss, centralized_images_by_category)\n",
    "\n",
    "# Resultados de PCA\n",
    "unet_pca_95 = pca_results[\"unet\"][\"95_variance\"]\n",
    "vgg_pca_95 = pca_results[\"vgg\"][\"95_variance\"]\n",
    "\n",
    "num_components_unet_95 = pca_results[\"num_components\"][\"unet\"][\"95_variance\"]\n",
    "num_components_vgg_95 = pca_results[\"num_components\"][\"vgg\"][\"95_variance\"]\n",
    "\n",
    "# Exibindo o n√∫mero de componentes reduzidos por categoria para U-Net e VGG\n",
    "print(\"Number of Components for U-Net (95% Variance):\", num_components_unet_95)\n",
    "print(\"Number of Components for VGG (95% Variance):\", num_components_vgg_95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components_ matrix has the shape (n_components, n_features), but when you project the original data into this new principal components space, the data is transformed into a shape matrix (n_samples, n_components)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_test_images(test_dir, categories, image_size, input_size=(224,224)):\n",
    "    test_images_by_category = load_images_by_category(test_dir, categories, image_size)\n",
    "    test_centralized_images_by_category = {}\n",
    "\n",
    "    for category, images in test_images_by_category.items():\n",
    "        test_centralized_images = center_images(images)\n",
    "        test_centralized_images_by_category[category] = test_centralized_images\n",
    "\n",
    "    return test_centralized_images_by_category\n",
    "\n",
    "test_preprocessed_images_by_category = load_and_preprocess_test_images('images_test', y, image_size=(224,224), input_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, images in centralized_images_by_category.items():\n",
    "    mean = check_centralization(images)\n",
    "    print(f\"Mean pixel values after centralization for category {category}: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para extrair as features de teste para UNet e VGG\n",
    "def extract_test_features(test_images_by_category, unet_model, vgg_model):\n",
    "    unet_model.eval()\n",
    "    vgg_model.eval()\n",
    "    \n",
    "    unet_features_by_category = {}\n",
    "    vgg_features_by_category = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for category, images in test_images_by_category.items():\n",
    "            unet_features_by_category[category] = []\n",
    "            vgg_features_by_category[category] = []\n",
    "\n",
    "            for i, image in enumerate(images):\n",
    "                # Preprocessar imagem\n",
    "                image_tensor = torch.tensor(image / 255.0).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "                # Extrair features da U-Net\n",
    "                reconstructed_image, unet_features = unet_model(image_tensor)\n",
    "                unet_features_combined = unet_features.view(-1).cpu().numpy()\n",
    "                unet_features_by_category[category].append(unet_features_combined)\n",
    "\n",
    "                # Extrair features da VGG\n",
    "                vgg_features = vgg_model(image_tensor)\n",
    "                vgg_features_combined = torch.cat([f.view(-1) for f in vgg_features], dim=0).cpu().numpy()\n",
    "                vgg_features_by_category[category].append(vgg_features_combined)\n",
    "\n",
    "    return unet_features_by_category, vgg_features_by_category\n",
    "\n",
    "# Carregar as imagens de teste pr√©-processadas\n",
    "# Sup√µe-se que test_preprocessed_images_by_category j√° foi carregado anteriormente\n",
    "\n",
    "# Extrair features das imagens de teste\n",
    "unet_test_features, vgg_test_features = extract_test_features(test_preprocessed_images_by_category, unet, vgg_loss)\n",
    "\n",
    "# Agora as features est√£o extra√≠das para as imagens de teste e est√£o armazenadas em 'unet_test_features' e 'vgg_test_features'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Fun√ß√£o para calcular residuals projetando as features no PCA\n",
    "def project_and_calculate_residuals(features, pca):\n",
    "    # Certifique-se de que as features s√£o 2D\n",
    "    if features.ndim == 1:\n",
    "        features = features.reshape(1, -1)\n",
    "    \n",
    "    # Projeta as features e calcula os residuals\n",
    "    projected_features = pca.inverse_transform(pca.transform(features))\n",
    "    residuals = features - projected_features\n",
    "    return residuals\n",
    "\n",
    "# Fun√ß√£o para calcular os OOD scores usando os residuals\n",
    "def calculate_ood_scores(residuals):\n",
    "    # O OOD score ser√° a m√©dia dos valores absolutos dos residuals\n",
    "    ood_scores = np.mean(np.abs(residuals), axis=1)\n",
    "    return np.mean(ood_scores)\n",
    "\n",
    "# Fun√ß√£o para processar os residuals e calcular o OOD score\n",
    "def process_and_calculate_ood(residuals_list):\n",
    "    total_residuals = np.vstack(residuals_list)\n",
    "    return calculate_ood_scores(total_residuals)\n",
    "\n",
    "# Fun√ß√£o para processar as features e calcular OOD ap√≥s PCA\n",
    "def process_and_calculate_ood_with_pca(features_list, pca_model):\n",
    "    residuals_list = []\n",
    "    \n",
    "    for features in features_list:\n",
    "        # Certifique-se de que cada conjunto de features seja um conjunto de amostras (n√£o um √∫nico vetor)\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1, -1)\n",
    "        \n",
    "        # Aplica PCA e calcula os residuals\n",
    "        residuals = project_and_calculate_residuals(features, pca_model)\n",
    "        residuals_list.append(residuals)\n",
    "    \n",
    "    return process_and_calculate_ood(residuals_list)\n",
    "\n",
    "# Treinando PCA com as features de treinamento (sem normaliza√ß√£o)\n",
    "def train_pca_on_features(features_list, n_components=0.95):\n",
    "    # Unindo todas as features de treinamento\n",
    "    all_features = np.vstack(features_list)\n",
    "    \n",
    "    # Aplicando PCA para capturar 95% da vari√¢ncia\n",
    "    pca_model = PCA(n_components=n_components)\n",
    "    pca_model.fit(all_features)\n",
    "    \n",
    "    return pca_model\n",
    "\n",
    "# Treinando PCA para as features da categoria 'Coast' e 'Office'\n",
    "pca_coast = train_pca_on_features(unet_test_features['Coast'], n_components=0.95)\n",
    "pca_office = train_pca_on_features(unet_test_features['Office'], n_components=0.95)\n",
    "\n",
    "# Calculando OOD scores projetando nos componentes do PCA\n",
    "ood_score_coast_train_coast_test = process_and_calculate_ood_with_pca(unet_test_features['Coast'], pca_coast)\n",
    "ood_score_coast_train_office_test = process_and_calculate_ood_with_pca(unet_test_features['Office'], pca_coast)\n",
    "ood_score_office_train_office_test = process_and_calculate_ood_with_pca(unet_test_features['Office'], pca_office)\n",
    "ood_score_office_train_coast_test = process_and_calculate_ood_with_pca(unet_test_features['Coast'], pca_office)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(\"OOD Scores usando Coast Training Data nos componentes do PCA - Coast Test Data:\")\n",
    "print(f\"OOD Score: {ood_score_coast_train_coast_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando Coast Training Data nos componentes do PCA - Office Test Data:\")\n",
    "print(f\"OOD Score: {ood_score_coast_train_office_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando Office Training Data nos componentes do PCA - Office Test Data:\")\n",
    "print(f\"OOD Score: {ood_score_office_train_office_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando Office Training Data nos componentes do PCA - Coast Test Data:\")\n",
    "print(f\"OOD Score: {ood_score_office_train_coast_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando PCA para as features da categoria 'Coast' e 'Office' com base nas features da VGG\n",
    "pca_coast = train_pca_on_features(vgg_test_features['Coast'], n_components=0.95)\n",
    "pca_office = train_pca_on_features(vgg_test_features['Office'], n_components=0.95)\n",
    "\n",
    "# Calculando OOD scores projetando nos componentes do PCA usando as features da VGG\n",
    "ood_score_coast_train_coast_test = process_and_calculate_ood_with_pca(vgg_test_features['Coast'], pca_coast)\n",
    "ood_score_coast_train_office_test = process_and_calculate_ood_with_pca(vgg_test_features['Office'], pca_coast)\n",
    "ood_score_office_train_office_test = process_and_calculate_ood_with_pca(vgg_test_features['Office'], pca_office)\n",
    "ood_score_office_train_coast_test = process_and_calculate_ood_with_pca(vgg_test_features['Coast'], pca_office)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(\"OOD Scores usando Coast Training Data nos componentes do PCA - Coast Test Data (VGG Features):\")\n",
    "print(f\"OOD Score: {ood_score_coast_train_coast_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando Coast Training Data nos componentes do PCA - Office Test Data (VGG Features):\")\n",
    "print(f\"OOD Score: {ood_score_coast_train_office_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando Office Training Data nos componentes do PCA - Office Test Data (VGG Features):\")\n",
    "print(f\"OOD Score: {ood_score_office_train_office_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando Office Training Data nos componentes do PCA - Coast Test Data (VGG Features):\")\n",
    "print(f\"OOD Score: {ood_score_office_train_coast_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úåÔ∏è Part II: Comparing two similar environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = ['Bedroom', 'LivingRoom']\n",
    "\n",
    "df_different = df[df['category'].isin(train_categories)]\n",
    "df_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_different['image_path']\n",
    "y = df_different['category']\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "image_size = (224, 224)\n",
    "unique_categories = list(df_different['category'].unique())\n",
    "print(f\"Unique categories: {unique_categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_images_set(X_train, X_test, y_train, y_test, output_dir_train='images_train', output_dir_test='images_test', standard_size=standard_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images_by_category = load_images_by_category('images_train', unique_categories, image_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_images_by_category = {}\n",
    "scalers_by_category = {}\n",
    "for category, images in training_images_by_category.items():\n",
    "    centralized_images = center_images(images)\n",
    "    centralized_images_by_category[category] = centralized_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_centralization(images):\n",
    "    mean = np.mean(images, axis=(0, 1, 2))\n",
    "    return mean\n",
    "\n",
    "for category, images in centralized_images_by_category.items():\n",
    "    mean = check_centralization(images)\n",
    "    print(f\"Mean pixel values after centralization for category {category}: {mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Clear GPU cache before running the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# UNet model modified to output features\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = self.contracting_block(in_channels, 64)\n",
    "        self.encoder2 = self.contracting_block(64, 128)\n",
    "        self.encoder3 = self.contracting_block(128, 256)\n",
    "        self.encoder4 = self.contracting_block(256, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self.expansive_block(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.expansive_block(256, 128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.expansive_block(128, 64)\n",
    "        self.decoder1 = self.final_block(64, out_channels)\n",
    "\n",
    "    def contracting_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(kernel_size=3, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_size=3, in_channels=out_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def expansive_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(kernel_size=3, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_size=3, in_channels=out_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def final_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(kernel_size=3, in_channels=in_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_size=3, in_channels=out_channels, out_channels=out_channels, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_size=1, in_channels=out_channels, out_channels=out_channels),\n",
    "            nn.Sigmoid()  # Adiciona ativa√ß√£o sigmoid para garantir que a sa√≠da fique entre [0, 1]\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def crop_and_concat(self, upsampled, bypass):\n",
    "        _, _, H, W = upsampled.size()\n",
    "        _, _, H_b, W_b = bypass.size()\n",
    "        if H_b != H or W_b != W:\n",
    "            bypass = F.interpolate(bypass, size=(H, W), mode='bilinear', align_corners=True)\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(F.max_pool2d(enc1, kernel_size=2, stride=2))\n",
    "        enc3 = self.encoder3(F.max_pool2d(enc2, kernel_size=2, stride=2))\n",
    "        enc4 = self.encoder4(F.max_pool2d(enc3, kernel_size=2, stride=2))\n",
    "\n",
    "        # Decoder path\n",
    "        dec4 = self.crop_and_concat(self.upconv4(enc4), enc3)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.crop_and_concat(self.upconv3(dec4), enc2)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.crop_and_concat(self.upconv2(dec3), enc1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.decoder1(dec2)\n",
    "\n",
    "        return dec1, enc4  # Retorna tanto a imagem reconstru√≠da quanto as features da pen√∫ltima camada\n",
    "\n",
    "# VGG-based perceptual loss (Feature Extractor)\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.slice1 = nn.Sequential(*vgg[:5])   # conv1_2\n",
    "        self.slice2 = nn.Sequential(*vgg[5:10]) # conv2_2\n",
    "        self.slice3 = nn.Sequential(*vgg[10:19])# conv3_4\n",
    "        self.slice4 = nn.Sequential(*vgg[19:28])# conv4_4\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Converte imagens grayscale para 3 canais duplicando o √∫nico canal\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "        h_relu1 = self.slice1(x)\n",
    "        h_relu2 = self.slice2(h_relu1)\n",
    "        h_relu3 = self.slice3(h_relu2)\n",
    "        h_relu4 = self.slice4(h_relu3)\n",
    "        return [h_relu1, h_relu2, h_relu3, h_relu4]\n",
    "\n",
    "# Fun√ß√£o de treinamento do modelo\n",
    "def train_unet(unet, data_loader, num_epochs=20):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(unet.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        unet.train()\n",
    "        epoch_loss = 0\n",
    "        for images, in data_loader:\n",
    "            images = images.to(device).float()\n",
    "            \n",
    "            # Normaliza√ß√£o para intervalo [0, 1]\n",
    "            images /= 255.0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_images, _ = unet(images)\n",
    "            loss = criterion(reconstructed_images, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(data_loader):.4f}')\n",
    "\n",
    "# Inicializa√ß√£o dos modelos UNet e VGG para Perceptual Loss\n",
    "unet = UNet().to(device)\n",
    "vgg_loss = VGGPerceptualLoss().to(device)\n",
    "\n",
    "# Defina o DataLoader aqui\n",
    "categories = centralized_images_by_category.keys()\n",
    "\n",
    "for category in categories:\n",
    "    images = centralized_images_by_category[category]\n",
    "\n",
    "    # Adiciona a dimens√£o do canal e cria o DataLoader\n",
    "    images = np.expand_dims(images, axis=1)  # Adiciona dimens√£o do canal\n",
    "    dataset = TensorDataset(torch.tensor(images, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Treinando o modelo U-Net por 10 √©pocas (ou ajuste conforme necess√°rio)\n",
    "    train_unet(unet, loader, num_epochs=20)\n",
    "\n",
    "    # Visualizando as primeiras imagens de cada categoria\n",
    "    unet.eval()\n",
    "    vgg_loss.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(3):  # Exibe 3 imagens de cada categoria\n",
    "            image = centralized_images_by_category[category][i]\n",
    "            \n",
    "            # Normaliza√ß√£o da imagem antes de passar pela UNet\n",
    "            image_tensor = torch.tensor(image / 255.0).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "            # Processa com UNet\n",
    "            reconstructed_image, unet_features = unet(image_tensor)\n",
    "\n",
    "            # Extrai features com o modelo VGG\n",
    "            vgg_features = vgg_loss(image_tensor)\n",
    "            vgg_features_combined = torch.cat([f.view(-1) for f in vgg_features], dim=0).cpu().numpy()\n",
    "\n",
    "            # Extrai as features da U-Net e as converte para numpy\n",
    "            unet_features_combined = unet_features.view(-1).cpu().numpy()\n",
    "\n",
    "            # Converte o tensor para numpy para visualiza√ß√£o\n",
    "            reconstructed_image_np = reconstructed_image.squeeze().cpu().numpy()\n",
    "\n",
    "            # Visualiza as imagens\n",
    "            plt.figure(figsize=(20, 5))\n",
    "            \n",
    "            # Imagem original\n",
    "            plt.subplot(1, 4, 1)\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.title(f'Original {category} Image {i+1}')\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Imagem reconstru√≠da\n",
    "            plt.subplot(1, 4, 2)\n",
    "            plt.imshow(reconstructed_image_np, cmap='gray')\n",
    "            plt.title(f'Reconstructed {category} Image {i+1}')\n",
    "            plt.axis('off')\n",
    "\n",
    "            # UNet Features\n",
    "            plt.subplot(1, 4, 3)\n",
    "            plt.plot(unet_features_combined)\n",
    "            plt.title(f'UNet Features {category} Image {i+1}')\n",
    "            plt.axis('on')\n",
    "\n",
    "            # VGG Features\n",
    "            plt.subplot(1, 4, 4)\n",
    "            plt.plot(vgg_features_combined)\n",
    "            plt.title(f'VGG Features {category} Image {i+1}')\n",
    "            plt.axis('on')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Fun√ß√£o para aplicar PCA reduzida\n",
    "def apply_reduced_pca(features_by_category, n_components=1024, number_variance=0.95):\n",
    "    pca_by_category = {}\n",
    "    num_components_reduced_dict = {}\n",
    "    \n",
    "    for category, features in features_by_category.items():\n",
    "        all_features = np.vstack(features)  # Agrupa todas as features da categoria em uma matriz\n",
    "        \n",
    "        # Verifica se h√° features para processar\n",
    "        if all_features.size == 0:\n",
    "            continue\n",
    "        \n",
    "        # Ajusta dinamicamente o n√∫mero de componentes baseado em min(n_samples, n_features)\n",
    "        n_samples, n_features = all_features.shape\n",
    "        max_components = min(n_samples, n_features)\n",
    "\n",
    "        # Ajusta a PCA com o n√∫mero inicial de componentes\n",
    "        pca = PCA(n_components=max_components)\n",
    "        pca.fit(all_features)\n",
    "        \n",
    "        # Determina o n√∫mero de componentes que ret√™m a vari√¢ncia desejada\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        num_components_reduced = np.where(cumulative_variance >= number_variance)[0][0] + 1\n",
    "        \n",
    "        # Reajusta a PCA para o n√∫mero reduzido de componentes\n",
    "        pca = PCA(n_components=num_components_reduced)\n",
    "        pca.fit(all_features)\n",
    "        \n",
    "        # Armazena a PCA e o n√∫mero de componentes reduzidos por categoria\n",
    "        pca_by_category[category] = pca\n",
    "        num_components_reduced_dict[category] = num_components_reduced\n",
    "\n",
    "    # Calcula o n√∫mero m√≠nimo de componentes entre todas as categorias\n",
    "    min_num_components = min(num_components_reduced_dict.values())\n",
    "    return pca_by_category, num_components_reduced_dict, min_num_components\n",
    "\n",
    "\n",
    "# Extrair features por categoria para UNet e VGG\n",
    "def extract_features_by_category(unet, vgg_loss, images_by_category):\n",
    "    unet_features_by_category = {}\n",
    "    vgg_features_by_category = {}\n",
    "\n",
    "    for category, images in images_by_category.items():\n",
    "        unet_features_by_category[category] = []\n",
    "        vgg_features_by_category[category] = []\n",
    "        \n",
    "        for image in images:\n",
    "            image_tensor = torch.tensor(image / 255.0).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "            # Extrai as features da U-Net\n",
    "            _, unet_features = unet(image_tensor)\n",
    "            unet_features_combined = unet_features.view(-1).detach().cpu().numpy()\n",
    "            unet_features_by_category[category].append(unet_features_combined)\n",
    "\n",
    "            # Extrai as features da VGG\n",
    "            vgg_features = vgg_loss(image_tensor)\n",
    "            vgg_features_combined = torch.cat([f.view(-1).detach() for f in vgg_features], dim=0).cpu().numpy()\n",
    "            vgg_features_by_category[category].append(vgg_features_combined)\n",
    "    \n",
    "    return unet_features_by_category, vgg_features_by_category\n",
    "\n",
    "\n",
    "# Fun√ß√£o principal para calcular a PCA reduzida para U-Net e VGG\n",
    "def calculate_pca_for_unet_and_vgg(unet, vgg_loss, images_by_category):\n",
    "    # Extrai as features da U-Net e do VGG por categoria\n",
    "    unet_features_by_category, vgg_features_by_category = extract_features_by_category(unet, vgg_loss, images_by_category)\n",
    "\n",
    "    # Aplica PCA reduzida para as features da U-Net e VGG com vari√¢ncia de 95%, 90%, 85%, e 80%\n",
    "    pca_unet_95, num_components_unet_95, _ = apply_reduced_pca(unet_features_by_category, number_variance=0.95)\n",
    "    pca_vgg_95, num_components_vgg_95, _ = apply_reduced_pca(vgg_features_by_category, number_variance=0.95)\n",
    "\n",
    "    pca_unet_90, num_components_unet_90, _ = apply_reduced_pca(unet_features_by_category, number_variance=0.90)\n",
    "    pca_vgg_90, num_components_vgg_90, _ = apply_reduced_pca(vgg_features_by_category, number_variance=0.90)\n",
    "\n",
    "    pca_unet_85, num_components_unet_85, _ = apply_reduced_pca(unet_features_by_category, number_variance=0.85)\n",
    "    pca_vgg_85, num_components_vgg_85, _ = apply_reduced_pca(vgg_features_by_category, number_variance=0.85)\n",
    "\n",
    "    pca_unet_80, num_components_unet_80, _ = apply_reduced_pca(unet_features_by_category, number_variance=0.80)\n",
    "    pca_vgg_80, num_components_vgg_80, _ = apply_reduced_pca(vgg_features_by_category, number_variance=0.80)\n",
    "\n",
    "    # Retorna as PCAs calculadas para U-Net e VGG\n",
    "    return {\n",
    "        \"unet\": {\n",
    "            \"95_variance\": pca_unet_95,\n",
    "            \"90_variance\": pca_unet_90,\n",
    "            \"85_variance\": pca_unet_85,\n",
    "            \"80_variance\": pca_unet_80\n",
    "        },\n",
    "        \"vgg\": {\n",
    "            \"95_variance\": pca_vgg_95,\n",
    "            \"90_variance\": pca_vgg_90,\n",
    "            \"85_variance\": pca_vgg_85,\n",
    "            \"80_variance\": pca_vgg_80\n",
    "        },\n",
    "        \"num_components\": {\n",
    "            \"unet\": {\n",
    "                \"95_variance\": num_components_unet_95,\n",
    "                \"90_variance\": num_components_unet_90,\n",
    "                \"85_variance\": num_components_unet_85,\n",
    "                \"80_variance\": num_components_unet_80\n",
    "            },\n",
    "            \"vgg\": {\n",
    "                \"95_variance\": num_components_vgg_95,\n",
    "                \"90_variance\": num_components_vgg_90,\n",
    "                \"85_variance\": num_components_vgg_85,\n",
    "                \"80_variance\": num_components_vgg_80\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Aplicando o PCA nas features da U-Net e VGG\n",
    "pca_results = calculate_pca_for_unet_and_vgg(unet, vgg_loss, centralized_images_by_category)\n",
    "\n",
    "# Resultados de PCA\n",
    "unet_pca_95 = pca_results[\"unet\"][\"95_variance\"]\n",
    "vgg_pca_95 = pca_results[\"vgg\"][\"95_variance\"]\n",
    "\n",
    "num_components_unet_95 = pca_results[\"num_components\"][\"unet\"][\"95_variance\"]\n",
    "num_components_vgg_95 = pca_results[\"num_components\"][\"vgg\"][\"95_variance\"]\n",
    "\n",
    "# Exibindo o n√∫mero de componentes reduzidos por categoria para U-Net e VGG\n",
    "print(\"Number of Components for U-Net (95% Variance):\", num_components_unet_95)\n",
    "print(\"Number of Components for VGG (95% Variance):\", num_components_vgg_95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_test_images(test_dir, categories, image_size, input_size=(224,224)):\n",
    "    test_images_by_category = load_images_by_category(test_dir, categories, image_size)\n",
    "    test_centralized_images_by_category = {}\n",
    "\n",
    "    for category, images in test_images_by_category.items():\n",
    "        test_centralized_images = center_images(images)\n",
    "        test_centralized_images_by_category[category] = test_centralized_images\n",
    "\n",
    "    return test_centralized_images_by_category\n",
    "\n",
    "test_preprocessed_images_by_category = load_and_preprocess_test_images('images_test', y, image_size=(224,224), input_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, images in centralized_images_by_category.items():\n",
    "    mean = check_centralization(images)\n",
    "    print(f\"Mean pixel values after centralization for category {category}: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Fun√ß√£o para extrair features por categoria da U-Net e VGG\n",
    "def extract_features_by_category(unet, vgg_loss, images_by_category):\n",
    "    unet_features_by_category = {}\n",
    "    vgg_features_by_category = {}\n",
    "\n",
    "    for category, images in images_by_category.items():\n",
    "        unet_features_by_category[category] = []\n",
    "        vgg_features_by_category[category] = []\n",
    "        \n",
    "        for image in images:\n",
    "            image_tensor = torch.tensor(image / 255.0).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "            # Extrai as features da U-Net\n",
    "            _, unet_features = unet(image_tensor)\n",
    "            unet_features_combined = unet_features.view(-1).detach().cpu().numpy()\n",
    "            unet_features_by_category[category].append(unet_features_combined)\n",
    "\n",
    "            # Extrai as features da VGG\n",
    "            vgg_features = vgg_loss(image_tensor)\n",
    "            vgg_features_combined = torch.cat([f.view(-1).detach() for f in vgg_features], dim=0).cpu().numpy()\n",
    "            vgg_features_by_category[category].append(vgg_features_combined)\n",
    "    \n",
    "    return unet_features_by_category, vgg_features_by_category\n",
    "\n",
    "# Simula√ß√£o de extra√ß√£o de features para imagens\n",
    "# Suponha que centralized_images_by_category j√° est√° definido e cont√©m suas imagens de teste\n",
    "unet_features_by_category, vgg_features_by_category = extract_features_by_category(unet, vgg_loss, centralized_images_by_category)\n",
    "\n",
    "# Fun√ß√£o para calcular residuals projetando as features no PCA\n",
    "def project_and_calculate_residuals(features, pca):\n",
    "    if features.ndim == 1:\n",
    "        features = features.reshape(1, -1)\n",
    "    \n",
    "    projected_features = pca.inverse_transform(pca.transform(features))\n",
    "    residuals = features - projected_features\n",
    "    return residuals\n",
    "\n",
    "# Fun√ß√£o para calcular os OOD scores usando os residuals\n",
    "def calculate_ood_scores(residuals):\n",
    "    ood_scores = np.mean(np.abs(residuals), axis=1)\n",
    "    return np.mean(ood_scores)\n",
    "\n",
    "# Fun√ß√£o para processar e calcular OOD ap√≥s PCA\n",
    "def process_and_calculate_ood_with_pca(features_list, pca_model):\n",
    "    residuals_list = []\n",
    "    \n",
    "    for features in features_list:\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1, -1)\n",
    "        \n",
    "        residuals = project_and_calculate_residuals(features, pca_model)\n",
    "        residuals_list.append(residuals)\n",
    "    \n",
    "    return calculate_ood_scores(np.vstack(residuals_list))\n",
    "\n",
    "# Fun√ß√£o para treinar PCA\n",
    "def train_pca_on_features(features_list, n_components=0.95):\n",
    "    all_features = np.vstack(features_list)\n",
    "    \n",
    "    # Aplicando PCA para capturar 95% da vari√¢ncia\n",
    "    pca_model = PCA(n_components=n_components)\n",
    "    pca_model.fit(all_features)\n",
    "    \n",
    "    return pca_model\n",
    "\n",
    "# Agora, usamos as features extra√≠das para calcular os PCA e OOD scores\n",
    "pca_bedroom = train_pca_on_features(unet_features_by_category['Bedroom'], n_components=0.95)\n",
    "pca_livingRoom = train_pca_on_features(unet_features_by_category['LivingRoom'], n_components=0.95)\n",
    "\n",
    "# Calculando os OOD scores\n",
    "ood_score_bedroom_train_bedroom_test = process_and_calculate_ood_with_pca(unet_features_by_category['Bedroom'], pca_bedroom)\n",
    "ood_score_bedroom_train_livingRoom_test = process_and_calculate_ood_with_pca(unet_features_by_category['LivingRoom'], pca_bedroom)\n",
    "ood_score_livingRoom_train_livingRoom_test = process_and_calculate_ood_with_pca(unet_features_by_category['LivingRoom'], pca_livingRoom)\n",
    "ood_score_livingRoom_train_bedroom_test = process_and_calculate_ood_with_pca(unet_features_by_category['Bedroom'], pca_livingRoom)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(\"OOD Scores usando bedroom Training Data nos componentes do PCA - bedroom Test Data:\")\n",
    "print(f\"OOD Score: {ood_score_bedroom_train_bedroom_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando bedroom Training Data nos componentes do PCA - livingRoom Test Data:\")\n",
    "print(f\"OOD Score: {ood_score_bedroom_train_livingRoom_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando livingRoom Training Data nos componentes do PCA - livingRoom Test Data:\")\n",
    "print(f\"OOD Score: {ood_score_livingRoom_train_livingRoom_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando livingRoom Training Data nos componentes do PCA - bedroom Test Data:\")\n",
    "print(f\"OOD Score: {ood_score_livingRoom_train_bedroom_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando PCA para as features da categoria 'bedroom' e 'livingRoom' com base nas features da VGG\n",
    "pca_bedroom = train_pca_on_features(vgg_features_by_category['Bedroom'], n_components=0.95)\n",
    "pca_livingRoom = train_pca_on_features(vgg_features_by_category['LivingRoom'], n_components=0.95)\n",
    "\n",
    "# Calculando OOD scores projetando nos componentes do PCA usando as features da VGG\n",
    "ood_score_bedroom_train_bedroom_test = process_and_calculate_ood_with_pca(vgg_features_by_category['Bedroom'], pca_bedroom)\n",
    "ood_score_bedroom_train_livingRoom_test = process_and_calculate_ood_with_pca(vgg_features_by_category['LivingRoom'], pca_bedroom)\n",
    "ood_score_livingRoom_train_livingRoom_test = process_and_calculate_ood_with_pca(vgg_features_by_category['LivingRoom'], pca_livingRoom)\n",
    "ood_score_livingRoom_train_bedroom_test = process_and_calculate_ood_with_pca(vgg_features_by_category['Bedroom'], pca_livingRoom)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(\"OOD Scores usando bedroom Training Data nos componentes do PCA - bedroom Test Data (VGG Features):\")\n",
    "print(f\"OOD Score: {ood_score_bedroom_train_bedroom_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando bedroom Training Data nos componentes do PCA - livingRoom Test Data (VGG Features):\")\n",
    "print(f\"OOD Score: {ood_score_bedroom_train_livingRoom_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando livingRoom Training Data nos componentes do PCA - livingRoom Test Data (VGG Features):\")\n",
    "print(f\"OOD Score: {ood_score_livingRoom_train_livingRoom_test}\")\n",
    "\n",
    "print(\"\\nOOD Scores usando livingRoom Training Data nos componentes do PCA - bedroom Test Data (VGG Features):\")\n",
    "print(f\"OOD Score: {ood_score_livingRoom_train_bedroom_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['image_path'].tolist()\n",
    "y = df['category'].tolist()\n",
    "unique_categories = list(df['category'].unique())\n",
    "print(f\"Unique categories: {unique_categories}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "standard_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_images_set(X_train, X_test, y_train, y_test, output_dir_train='images_train', output_dir_test='images_test', standard_size=standard_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preprocessed_images_by_category = load_and_preprocess_test_images('images_test', y, image_size, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_features_by_category = extract_features_with_vgg16(model, all_test_preprocessed_images_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_preprocessed_images_by_category = load_and_preprocess_test_images('images_train', y, image_size, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_features_by_category = extract_features_with_vgg16(model, all_training_preprocessed_images_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_by_category = {}\n",
    "explained_variance_by_category = {}\n",
    "\n",
    "for category, features in all_training_features_by_category.items():\n",
    "    pca = PCA(n_components=0.95)  \n",
    "    principal_components = pca.fit_transform(features)\n",
    "    pca_by_category[category] = pca\n",
    "    explained_variance_by_category[category] = pca.explained_variance_ratio_\n",
    "    \n",
    "    print(f\"Category {category}, principal components: {principal_components.shape[1]}\")\n",
    "\n",
    "for category, pca in pca_by_category.items():\n",
    "    print(f\"Category {category}, principal components shape: {pca.components_.shape}\")\n",
    "    print(f\"Category {category}, explained variance: {np.sum(explained_variance_by_category[category]) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reconstruction_error(test_features, pca_by_category):\n",
    "    reconstruction_errors_by_category = {}\n",
    "    mean_reconstruction_errors_by_category = {}\n",
    "    \n",
    "    for category, pca in pca_by_category.items():\n",
    "        principal_components = pca.transform(test_features)\n",
    "        reconstructed_features = pca.inverse_transform(principal_components)\n",
    "        \n",
    "        reconstruction_error = np.linalg.norm(test_features - reconstructed_features, axis=1)\n",
    "        reconstruction_errors_by_category[category] = reconstruction_error / np.linalg.norm(test_features)\n",
    "\n",
    "    for category, errors in reconstruction_errors_by_category.items():\n",
    "        mean_reconstruction_errors_by_category[category] = np.mean(errors)\n",
    "    \n",
    "    best_category = min(mean_reconstruction_errors_by_category, key=mean_reconstruction_errors_by_category.get)\n",
    "\n",
    "    for category in mean_reconstruction_errors_by_category:\n",
    "        print(f\"Category {category}, mean reconstruction error: {mean_reconstruction_errors_by_category[category]}\")\n",
    "    \n",
    "    print(f\"Best category: {best_category}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    return mean_reconstruction_errors_by_category, best_category\n",
    "\n",
    "for category, test_features in all_test_features_by_category.items():\n",
    "    print(f\"Test category: {category}\")\n",
    "    mean_reconstruction_errors, best_category = calculate_reconstruction_error(test_features, pca_by_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "true_categories = []\n",
    "predicted_categories = []\n",
    "\n",
    "for true_category, test_features in all_test_features_by_category.items():\n",
    "    print(f\"True category: {true_category}\")\n",
    "    mean_reconstruction_errors, best_category = calculate_reconstruction_error(test_features, pca_by_category)\n",
    "    true_categories.append(true_category)\n",
    "    predicted_categories.append(best_category)\n",
    "\n",
    "labels = list(pca_by_category.keys())\n",
    "cm = confusion_matrix(true_categories, predicted_categories, labels=labels)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "cm_display.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(pca_by_category.keys())\n",
    "confusion_matrix = np.zeros((len(labels), len(labels)))\n",
    "\n",
    "for true_category, test_features in all_test_features_by_category.items():\n",
    "    mean_reconstruction_errors, _ = calculate_reconstruction_error(test_features, pca_by_category)\n",
    "    true_idx = labels.index(true_category)\n",
    "    \n",
    "    for pred_category, error in mean_reconstruction_errors.items():\n",
    "        pred_idx = labels.index(pred_category)\n",
    "        confusion_matrix[true_idx, pred_idx] = error\n",
    "\n",
    "max_error = confusion_matrix.max()\n",
    "confusion_matrix_normalized = confusion_matrix / max_error\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "cax = ax.matshow(confusion_matrix_normalized, cmap=plt.cm.Blues)\n",
    "plt.colorbar(cax)\n",
    "\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_yticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=90)\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "plt.title('Discrimination Matrix with Mean Reconstruction Errors')\n",
    "plt.xlabel('Latent Space')\n",
    "plt.ylabel('Projection/Eigen Space')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
